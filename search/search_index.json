{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<ul> <li> <p> Set up in 1 minute</p> <p>Install <code>fits2db</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> Contribute</p> <p>Are you missing something or found a bug?</p> <p> Contribute</p> </li> <li> <p> Focus on your data</p> <p>You're tired of managing hundrets of fits files on your computer? Read about why this project is for you!</p> <p> About this project</p> </li> <li> <p> Open Source, Apache License 2.0</p> <p>fits2db is licensed under Apache License 2.0 and available on GitHub</p> <p> LICENSE</p> </li> </ul>"},{"location":"contribution/code/","title":"Contribute to the code","text":""},{"location":"contribution/code/#getting-started","title":"Getting Started","text":"<p>To build the package and install the requirements you can run in the root directory <pre><code>pip install .\n</code></pre></p>"},{"location":"contribution/code/#setup-the-testing-environment","title":"Setup the testing environment","text":"<p>To be sure the new code don't break something older please run the tests before comitting the code. If your code breaks the tests we have to reject the merge request. To run the tests the easy way is to use docker:</p>"},{"location":"contribution/code/#docker-setup","title":"Docker setup","text":"<p>For the docker setup you can follow these steps:</p> <ul> <li>Open your terminal where you have docker </li> <li>Change to the folder <code>tests/integration/mysql</code> (for mysql tests)</li> <li>Run in the terminal      <pre><code>docker-compose up -d # This starts a mysql database for testing\n</code></pre></li> </ul>"},{"location":"contribution/code/#run-tests","title":"Run tests","text":"<p>Change back to the root folder and run  <pre><code>pytest\n</code></pre> this will collect all tests. If you want to have some information about code coverage run  <pre><code>coverage run -m pytest\n</code></pre> to show the results you can run either of these </p> TerminalDetailed html <pre><code>coverage report\n</code></pre> <pre><code>coverage html\n</code></pre>"},{"location":"contribution/contribution/","title":"Contributing to fits2db","text":"<p>Thank you for your interest in contributing to fits2db! We value the contributions from our community, whether they're big or small.</p>"},{"location":"contribution/contribution/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways you can contribute to our project:</p> <ul> <li>Bug Reports: Report issues you encounter on our Issues page.</li> <li>Feature Requests: Suggest new features or improvements in our Issues page.</li> <li>Start a discussion: If you have some discussion point or suggestions you can start a Discussion.</li> <li>Documentation: Help us improve or expand our documentation.</li> <li>Code: Contribute bug fixes or implement new features.</li> </ul>"},{"location":"contribution/contribution/#version-strategy","title":"Version Strategy","text":"<p>When contributing to fits2db, it's important to be aware of our versioning system, which follows semantic versioning principles:</p> <ul> <li>Patch Changes (0.0.x): For small updates that do not introduce new features or significant modifications, such as typo fixes, minor documentation updates, or small bug fixes.</li> <li>Minor Changes (0.x.0): For more substantial changes that add new features, improvements, or updates that do not break backward compatibility or change the public API.</li> <li>Major Changes (x.0.0): For major updates that introduce significant changes, such as breaking changes to the public API, large-scale refactors, or when the project reaches a stable production-ready release.</li> </ul> <p>By adhering to this versioning system, we can maintain clarity and predictability for users of our project, helping them understand the impact of each new release.</p>"},{"location":"contribution/docs/","title":"Contributing to fits2db Documentation","text":""},{"location":"contribution/docs/#getting-started","title":"Getting Started","text":"<p>Before you begin, make sure you have the necessary tools installed with : <pre><code>pip install -r mkdocs-requirements.txt\npip install fits2db\n</code></pre></p>"},{"location":"contribution/docs/#contributing-to-the-user-guides","title":"Contributing to the User guides","text":"<p>If you're only changing the user guide, tutorial, or any other non-code-related documentation:</p> <ol> <li>Navigate to the docs directory where the Markdown files for the user guide and other documentation are located.</li> <li>Make your changes or additions directly to the relevant Markdown (.md) files.</li> <li> <p>Test your changes locally to ensure everything looks correct: <pre><code>mkdocs serve\n</code></pre> Open http://127.0.0.1:8000 in your web browser to view the documentation site with your changes.</p> </li> <li> <p>Commit the code and create an merge request</p> </li> </ol>"},{"location":"contribution/docs/#contributing-to-the-code-documentation","title":"Contributing to the Code Documentation","text":"<p>If you're contributing to the code documentation:</p> <ol> <li>Follow the same steps to fork, clone, and set up the repository.</li> <li>You will need to install fits2db from source to see your changes made to the cli docs</li> <li>The code documentation is generated using mkdocstrings. To modify the documentation, Update the docstrings in your Python code following the project's conventions. Ensure that your docstrings are clear, concise, and make proper use of type annotations.</li> <li>Reinstall the package and run the <code>mkdocs serve</code> command to check if changes look good before you create a merge request.</li> </ol>"},{"location":"contribution/setup/","title":"Getting Started","text":"<p>To contribute to fits2db, follow these steps:</p> <ol> <li>Fork the repository by clicking \"Fork\" on the top right of this page.</li> <li>Clone your fork to your local machine</li> <li>Set up your local development environment. </li> </ol>"},{"location":"contribution/setup/#docs-setup","title":"Docs setup","text":"<pre><code>pip install -r mkdocs-requirements.txt\npip install fits2db\n</code></pre>"},{"location":"contribution/setup/#pytest-setup","title":"pytest setup","text":"<p>In the root folder and run  <pre><code>pip install -r test-requirements.txt\n</code></pre></p>"},{"location":"contribution/setup/#docker-setup","title":"Docker setup","text":"<p>For the docker setup you can follow these steps:</p> <ul> <li>Open your terminal where you have docker </li> <li>Change to the folder <code>tests/integration/mysql</code> (for mysql tests)</li> <li>Run in the terminal      <pre><code>docker-compose up -d # This starts a mysql database for testing\n</code></pre></li> </ul>"},{"location":"contribution/tests/","title":"Testing the code","text":"<p>We doint aim for 100% code coverage to have 100%. But it is good to have the basics covered. Thus we use tests to check if the interfaces between the modules work as expected.</p> <p>If you contribute new code it will run with unit-testing and if they are failing the code is not merged so please give it a run before opening a merge request </p>"},{"location":"contribution/tests/#setup-the-testing-environment","title":"Setup the testing environment","text":"<p>To be sure the new code don't break something older please run the tests before comitting the code. If your code breaks the tests we have to reject the merge request. To run the tests the easy way is to use docker:</p>"},{"location":"contribution/tests/#pytest-setup","title":"pytest setup","text":"<p>Change back to the root folder and run  <pre><code>pip install -r test-requirements.txt\n</code></pre></p>"},{"location":"contribution/tests/#docker-setup","title":"Docker setup","text":"<p>For the docker setup you can follow these steps:</p> <ul> <li>Open your terminal where you have docker </li> <li>Change to the folder <code>tests/integration/mysql</code> (for mysql tests)</li> <li>Run in the terminal      <pre><code>docker-compose up -d # This starts a mysql database for testing\n</code></pre></li> </ul>"},{"location":"contribution/tests/#run-tests","title":"Run tests","text":"<p>Change back to the root folder and run  <pre><code>pytest\n</code></pre> this will collect all tests. If you want to have some information about code coverage run  <pre><code>coverage run -m pytest\n</code></pre> to show the results you can run either of these </p> TerminalDetailed html <pre><code>coverage report\n</code></pre> <pre><code>coverage html\n</code></pre> <p>The tests of the base.py script are excluded in the github actions, but will run when pytest is called locally.</p>"},{"location":"reference/adapters_base/","title":"ADAPTERS.BASE Module","text":"<p>This module defines the BaseLoader class, an abstract base class for handling the loading of data from FITS files into a database. The class provides functionality to write metadata about the FITS files and their corresponding tables into the database and offers abstract methods for custom data operations such as dropping tables and upserting data.</p> <p>Classes:</p> Name Description <code>BaseLoader</code> <p>An abstract base class for writing data from FITS files into a database.</p>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for writing data from FITS files into a database.</p> <p>Attributes:</p> Name Type Description <code>db_url</code> <code>str</code> <p>The database URL.</p> <code>engine</code> <code>Engine</code> <p>The SQLAlchemy engine for the database.</p> <code>config</code> <code>ConfigType</code> <p>Configuration data for loading tables from the FITS file.</p> <code>file</code> <code>FitsFile</code> <p>The FITS file object containing data to be loaded.</p> <code>session</code> <code>Session</code> <p>SQLAlchemy session object for database transactions.</p> <code>new_file</code> <code>Fits2DbMeta</code> <p>Metadata object for the FITS file.</p> <code>db_table_names</code> <code>set</code> <p>Set of table names currently in the database.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>class BaseLoader(ABC):\n    \"\"\"\n    An abstract base class for writing data from FITS files into a database.\n\n    Attributes:\n        db_url (str): The database URL.\n        engine (engine.Engine): The SQLAlchemy engine for the database.\n        config (ConfigType): Configuration data for loading tables from the FITS file.\n        file (FitsFile): The FITS file object containing data to be loaded.\n        session (Session): SQLAlchemy session object for database transactions.\n        new_file (Fits2DbMeta): Metadata object for the FITS file.\n        db_table_names (set): Set of table names currently in the database.\n    \"\"\"\n\n    def __init__(\n        self, db_url: str, engine: engine, config: ConfigType, file: FitsFile\n    ):\n        \"\"\"\n        Initializes the BaseLoader with the given database URL, engine, configuration, and FITS file.\n\n        Args:\n            db_url (str): The database URL.\n            engine (engine.Engine): The SQLAlchemy engine for the database.\n            config (ConfigType): Configuration data for loading tables from the FITS file.\n            file (FitsFile): The FITS file object containing data to be loaded.\n        \"\"\"\n        self.db_url = db_url\n        self.engine = engine\n        self.config = config\n        self.file = file\n\n    @abstractmethod\n    def create_db_url(self) -&gt; str:\n        pass\n\n    def db_session(self) -&gt; Session:\n        \"\"\"\n        Creates and returns a new SQLAlchemy session for the database.\n\n        Returns:\n            Session: A new SQLAlchemy session object.\n        \"\"\"\n        Base.metadata.create_all(self.engine)\n        Session = sessionmaker(bind=self.engine)\n        return Session()\n\n    def write_file_meta(self, session: Session) -&gt; None:\n        \"\"\"\n        Writes metadata about the FITS file to the database.\n        \"\"\"\n        log.debug(f\"Filepath {self.file.absolute_path.as_posix()}\")\n        self.new_file = Fits2DbMeta(\n            filename=self.file.file_name,\n            filepath=self.file.absolute_path.as_posix(),\n            last_file_mutation=self.file.mdate,\n        )\n        session.add(self.new_file)\n        session.commit()\n\n    def write_table_meta(\n        self, tbl_name: str, df: pd.DataFrame, session: Session, file_id: int\n    ) -&gt; None:\n        \"\"\"\n        Writes metadata about a table in the FITS file to the database.\n\n        Args:\n            tbl_name (str): The name of the table.\n            df (pd.DataFrame): The DataFrame representing the table data.\n        \"\"\"\n        rows, cols = df.shape\n        table = session.execute(select(Fits2DbTableMeta).filter_by(file_meta_id=file_id, tablename=tbl_name)).scalar_one_or_none()\n        if table is None:\n            new_table = Fits2DbTableMeta(\n                file_meta_id=file_id,\n                tablename=str.lower(tbl_name),\n                record_count=rows,\n                column_count=cols,\n            )\n            session.add(new_table)\n        else:\n            table.record_count = rows\n            table.column_count = cols\n            table.tablename = str.lower(tbl_name)\n        session.commit()\n\n        # with self.engine.connect() as conn:\n            # transaction = conn.begin()\n            # try:\n                # if file_id is not None:\n                    # delete_stmt = (\n                        # delete(original_table_obj)\n                        # .where(original_table_obj.c.file_meta_id == file_id)\n                    # )\n                    # res = conn.execute(delete_stmt)\n\n    def get_tables(self, session: Session) -&gt; set[str]:\n        \"\"\"\n        Retrieves and stores the names of all tables currently in the database.\n        \"\"\"\n        db_table_names = session.execute(\n            select(Fits2DbTableMeta.tablename)\n        ).fetchall()\n        db_table_names = [name[0] for name in db_table_names]\n        return set(db_table_names)\n\n    def drop_user_tables(self, session: Session) -&gt; None:\n        \"\"\"\n        Drops FITS2DB created data tables from the database if they exist.\n        \"\"\"\n        metadata = MetaData()\n        metadata.reflect(bind=self.engine)\n        log.info(metadata.tables)\n        try:\n            # db_table_names = self.get_tables(session)\n            # Bugfix that no tables were found\n            db_table_names = set(metadata.tables.keys())\n            for table_name in db_table_names:\n                if table_name in metadata.tables:\n                    metadata.tables[table_name].drop(self.engine)\n                    log.info(f\"Dropped table {table_name}\")\n                if table_name + \"_META\" in metadata.tables:\n                    metadata.tables[table_name + \"_META\"].drop(self.engine)\n                    log.info(f\"Dropped table {table_name+'_META'}\")\n                if \"TMP_\" + table_name in metadata.tables:\n                    metadata.tables[\"TMP_\" + table_name].drop(self.engine)\n                    log.info(f\"Dropped table {'TMP_'+table_name}\")\n\n        except SQLAlchemyError as e:\n            log.error(f\"An error occurred while dropping tables: {e}\")\n        finally:\n            self.engine.dispose()\n\n    def delete_meta_tables(self, session: Session) -&gt; None:\n        \"\"\"\n        Drops FITS2DB created data tables from the database if they exist.\n        \"\"\"\n        log.debug(\"Start deletion of Meta tables\")\n        try:\n            session.query(Fits2DbTableMeta).delete()\n            log.debug(\"Run delete stmt for Fits2DbTableMeta\")\n            session.query(Fits2DbMeta).delete()\n            log.debug(\"Commit changes\")\n            session.commit()\n        except SQLAlchemyError as err:\n            log.error(err)\n\n    def clean_db(self) -&gt; None:\n        \"\"\"\n        Cleans the database by dropping specific tables and metadata tables.\n        \"\"\"\n        with self.db_session() as session:\n        # Easyer way to drop tables\n            meta = MetaData()\n            meta.reflect(bind=self.engine)\n            for tbl in reversed(meta.sorted_tables):\n                tbl.drop(self.engine)\n\n    def get_fits2db_meta(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieves the FITS2DB_META table from the database and returns it as a DataFrame.\n\n        Returns:\n            pd.DataFrame: The DataFrame containing the FITS2DB_META table data.\n        \"\"\"\n        try:\n            df = pd.read_sql_table(\"fits2db_meta\", con=self.engine)\n            return df\n        except Exception as err:\n            log.error(err)\n            raise\n\n    def upload_file(self) -&gt; None:\n        \"\"\"\n        Upserts the FITS file and its tables into the database.\n        \"\"\"\n        with self.db_session() as session:\n            self.write_file_meta(session)\n            table_configs = self.config[\"fits_files\"][\"tables\"]\n            log.debug(\"Start upserting data\")\n\n            updated_tables = []\n            faulty_tables = []\n            new_tables = []\n\n            for table in table_configs:\n                log.debug(f\"Table in configs: {table}\")\n                table_name = table[\"name\"]\n                log.info(table_name)\n                log.info(table[\"ingest_all_columns\"])\n                try:\n                    df = self.file.get_table(table_name)\n                    table_name = str.lower(table_name)\n                    df.data[\"FILE_META_ID\"] = self.new_file.id\n                    df.data.columns = map(str.lower, df.data.columns) # change to lower\n                    df.meta.columns = map(str.lower, df.meta.columns) # change to lower\n                    date_column = table[\"date_column\"]\n                    try:\n                        df.data = self._prepare_dataframe(df.data, date_column)\n                    except ValueError:\n                        faulty_tables.append((table_name, date_column))\n                        continue\n\n                    self.upsert_data_table(table_name, df.data)\n                    if self.check_table_exists(table_name):\n                        source_table_details = self._fetch_column_details('tmp_' + table_name)\n                        target_table_details = self._fetch_column_details(table_name)\n                        source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n                        new_columns = self._add_missing_columns(\n                            source_table_details, table_name, target_table_details\n                        )\n\n                        updated_tables.append((table_name, df, new_columns))\n                    else:\n                        new_tables.append((table_name, df))\n                    continue\n\n                except KeyError as err:\n                    # log.error(f\"\\n {err}\")\n                    log.warning(err.args[0])\n\n            if len(faulty_tables) &gt; 0:\n                log.error(f'Could not upload File {self.file.file_path}')\n                for table_name, date_column in faulty_tables:\n                    log.error(\n                        f'Error while parsing datetime column {date_column} in table {table_name}'\n                    )\n                self._delete_columns(updated_tables)\n                session.delete(self.new_file)\n                session.commit()\n                for table, df, new_columns in updated_tables: \n                    self.drop_table('tmp_' + table)\n                return\n            with self.engine.connect() as conn:\n                transaction = conn.begin()\n                try: \n                    for table, df, new_columns in updated_tables: \n                        self.merge_tables(table, 'tmp_' + table, conn)\n                    transaction.commit()\n                except Exception as e:\n                    transaction.rollback()  # Rollback the transaction on error\n                        # TODO delete file entry and exit function\n                    self._delete_columns(updated_tables)\n                    log.error(f'Could not Upload {self.file.file_path}')\n                    log.error(f\"An error occurred: {e}\")\n                    session.delete(self.new_file)\n                    session.commit()\n                    for table, df, new_columns in updated_tables: \n                        self.drop_table('tmp_' + table)\n                    return\n            for table, df, new_columns in updated_tables: \n                self.drop_table('tmp_' + table)\n                self.update_table(str.lower(table) + \"_meta\", df.meta) # change to lower\n                self.write_table_meta(\n                    table, df.data, session, self.new_file.id\n                )\n            for table, df in new_tables: \n                self.rename_table('tmp_' + table, table)\n                self.update_table(str.lower(table) + \"_meta\", df.meta) # change to lower\n                self.write_table_meta(\n                    table, df.data, session, self.new_file.id\n                )\n\n        # self.write_file_meta(session)\n\n    def update_fits2db_meta(self, session: Session) -&gt; Fits2DbMeta:\n        file_record = (\n            session.query(Fits2DbMeta)\n            .filter_by(\n                filepath=self.file.absolute_path.as_posix(),\n                filename=self.file.file_name,\n            )\n            .first()\n        )\n        if file_record is None:\n            log.error(\n                f\"No record found for file: {self.file.file_name} at {self.file.absolute_path.as_posix()}\"\n            )\n            return\n        # file_record.last_file_mutation = self.file.mdate\n        return file_record\n\n    def get_current_file_tables(self, session: Session, file_record: Fits2DbMeta):\n        tables_to_delete = session.query(Fits2DbTableMeta).filter(\n            Fits2DbTableMeta.file_meta_id == file_record.id\n        )\n        tables = {}\n        for table_meta in tables_to_delete:\n            tablename = table_meta.tablename\n            metadata = MetaData()\n            table = Table(tablename, metadata, autoload_with=self.engine)\n            tables[tablename] = table\n        return tables\n\n    def delete_file_from_table(self, session: Session, file_record: Fits2DbMeta, table: Table):\n        tables_to_delete = session.query(Fits2DbTableMeta).filter(\n            Fits2DbTableMeta.file_meta_id == file_record.id, Fits2DbTableMeta.tablename == table.name\n        )\n        tables = {}\n        delete_stmt = table.delete().where(\n            table.c.file_meta_id == file_record.id # change to lowercase\n        )\n        session.execute(delete_stmt)\n        log.info(\n            f\"Deleted rows in table '{table.name}' where file_meta_id = {file_record.id}\"\n        )\n        tables_to_delete.delete(synchronize_session=False)\n        return tables\n\n\n    def update_fits2db_table(self, session: Session, file_record: Fits2DbMeta):\n        \"\"\" Delete entries from each table, that belong to a specific file\n\n        Args:\n            session (Session): _description_\n            file_record (Fits2DbMeta): file meta data from the file whose data is to be deleted\n        \"\"\"\n        tables_to_delete = session.query(Fits2DbTableMeta).filter(\n            Fits2DbTableMeta.file_meta_id == file_record.id\n        )\n        for table_meta in tables_to_delete:\n            tablename = table_meta.tablename\n            metadata = MetaData()\n            table = Table(tablename, metadata, autoload_with=self.engine)\n            delete_stmt = table.delete().where(\n                table.c.file_meta_id == file_record.id # change to lowercase\n            )\n            session.execute(delete_stmt)\n            log.info(\n                f\"Deleted rows in table '{tablename}' where file_meta_id = {file_record.id}\"\n            )\n\n        tables_to_delete.delete(synchronize_session=False)\n\n    def update_file(self) -&gt; None:\n        \"\"\"\n        Updates the metadata and data of the FITS file in the database.\n        \"\"\"\n        with self.db_session() as session:\n            file_record = self.update_fits2db_meta(session)\n            remaining_tables = self.get_current_file_tables(session, file_record)\n            # self.update_fits2db_table(session, file_record)\n            session.commit()\n            table_configs = self.config[\"fits_files\"][\"tables\"]\n            log.debug(\"Start upserting data\")\n            updated_tables = []\n            faulty_tables = []\n            new_tables = []\n            for table in table_configs:\n                log.debug(f\"Table in configs: {table}\")\n                table_name = table[\"name\"]\n                log.info(table_name)\n                log.info(table[\"ingest_all_columns\"])\n                try:\n                    df = self.file.get_table(table_name)\n                    table_name = str.lower(table_name)\n                    df.data[\"FILE_META_ID\"] = file_record.id\n                    df.data.columns = map(str.lower, df.data.columns)\n                    df.meta.columns = map(str.lower, df.meta.columns)\n                    date_column = table[\"date_column\"]\n                    try:\n                        df.data = self._prepare_dataframe(df.data, date_column)\n                    except ValueError as err:\n                        faulty_tables.append((table_name, date_column))\n                        continue\n\n                    self.upsert_data_table(table_name, df.data, file_record.id)\n                    remaining_tables.pop(table_name, None)\n                    if self.check_table_exists(table_name):\n                        source_table_details = self._fetch_column_details('tmp_' + table_name)\n                        target_table_details = self._fetch_column_details(table_name)\n                        source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n                        new_columns = self._add_missing_columns(\n                            source_table_details, table_name, target_table_details\n                        )\n\n                        updated_tables.append((table_name, df, file_record.id, new_columns))\n                    else:\n                        new_tables.append((table_name, df, file_record.id))\n                    continue\n\n                except KeyError as err:\n                    # log.error(f\"\\n {err}\")\n                    log.warning(err.args[0])\n\n            if len(faulty_tables) &gt; 0:\n                log.error(f'Could not update File {self.file.file_path}')\n                for table_name, date_column in faulty_tables:\n                    log.error(\n                        f'Error while parsing datetime column {date_column} in table {table_name}'\n                    )\n                self._delete_columns(updated_tables)\n                session.commit()\n                for table, df, file_id, _ in updated_tables: \n                    self.drop_table('tmp_' + table)\n                return\n\n            with self.engine.connect() as conn:\n                transaction = conn.begin()\n                try: \n                    for table, df, file_id, new_columns in updated_tables: \n                        self.merge_tables(table, 'tmp_' + table, conn, file_id)\n                    transaction.commit()\n                except Exception as e:\n                    transaction.rollback()  # Rollback the transaction on error\n                    self._delete_columns(updated_tables)\n                    # TODO whatabout da file meta STUFF\n                    log.error(f\"An error occurred: {e}\")\n                    for table, df, file_id, _ in updated_tables: \n                        self.drop_table('tmp_' + table)\n                    return\n            for table, df, file_id, _ in updated_tables: \n                self.drop_table('tmp_' + table)\n                self.write_table_meta(\n                    table, df.data, session, file_record.id\n                )\n                self.update_table(table + \"_META\", df.meta)\n            for table, df, file_id in new_tables: \n                self.rename_table('tmp_' + table, table)\n                self.write_table_meta(\n                    table, df.data, session, file_record.id\n                )\n                self.update_table(table + \"_META\", df.meta)\n\n            if self.config['fits_files']['delete_rows_from_missing_tables']:\n                for k, table in remaining_tables.items():\n                    self.delete_file_from_table(session, file_record, table)\n\n            file_record.last_file_mutation = self.file.mdate\n            session.commit()\n\n    def upsert_data_table(self, table_name: str, df: pd.DataFrame, file_id: int=None) -&gt; None:\n        \"\"\"\n        Upserts data into a table in the database. If the table exists, merges the data.\n        Otherwise, renames the temporary table.\n\n        Args:\n            table_name (str): The name of the table to upsert.\n            df (pd.DataFrame): The DataFrame containing the data to upsert.\n        \"\"\"\n        log.debug(\"Passed engine:\")\n        log.debug(self.engine)\n\n        log.debug('clean df')  # TODO Extract\n\n\n        try:\n            tmp_tbl = \"tmp_\" + str.lower(table_name)  # change to lowercase\n            with self.engine.connect() as conn:\n                df.to_sql(\n                    name=tmp_tbl,\n                    con=conn,\n                    if_exists=\"replace\",\n                    index=False,\n                )\n                log.info(f\"Temporary table {tmp_tbl} created.\")\n\n            # if self.check_table_exists(table_name):\n                # self.merge_tables(table_name, tmp_tbl, file_id)\n                # self.drop_table(tmp_tbl)\n            # else:\n                # self.rename_table(tmp_tbl, table_name)\n\n        except Exception as err:\n            log.error(err)\n            raise\n\n    def check_table_exists(self, table_name: str) -&gt; bool:\n        \"\"\"\n        Checks if a table exists in the database.\n\n        Args:\n            table_name (str): The name of the table to check.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n        with self.engine.connect() as conn:\n            query = text(\"SHOW TABLES LIKE :table_name\")\n            result = conn.execute(query, {\"table_name\": table_name})\n            return result.fetchone() is not None\n\n    def drop_table(self, table_name: str) -&gt; bool:\n        \"\"\"\n        Drops a table from the database.\n\n        Args:\n            table_name (str): The name of the table to drop.\n\n        Returns:\n            bool: True if the table was successfully dropped, False otherwise.\n        \"\"\"\n        with self.engine.connect() as conn:\n            transaction = conn.begin()  # Start a new transaction\n            try:\n                # Safely create the SQL string with the table name included\n                query = text(f\"DROP TABLE `{table_name}`\")\n                conn.execute(query)\n                transaction.commit()  # Commit the transaction if the drop is successful\n                return True\n            except Exception as e:\n                transaction.rollback()  # Roll back the transaction on error\n                print(f\"Failed to drop table {table_name}: {e}\")\n                return False\n\n    def rename_table(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"\n        Renames a table in the database and adds an auto-incrementing primary key.\n\n        Args:\n            old_name (str): The current name of the table.\n            new_name (str): The new name for the table.\n        \"\"\"\n        with self.engine.connect() as conn:\n            try:\n                rename_stmt = text(f\"RENAME TABLE {old_name} TO {new_name}\")\n                id_stmt = text(f\"\"\"ALTER TABLE {new_name} \n                                ADD COLUMN id INT AUTO_INCREMENT,\n                                ADD PRIMARY KEY (id);\"\"\")\n                conn.execute(rename_stmt)\n                conn.execute(id_stmt)\n                log.info(\n                    f\"Table renamed from {old_name} to {new_name} and added primamry key id.\"\n                )\n            except SQLAlchemyError as err:\n                log.error(err)\n                raise\n\n    def execute_sql(self, sql: str) -&gt; None:\n        \"\"\"\n        Executes a raw SQL query against the database.\n\n        Args:\n            sql (str): The SQL query to execute.\n        \"\"\"\n        with self.engine.connect() as conn:\n            try:\n                conn.execute(text(sql))\n                log.info(\"Query executed successfully\")\n            except SQLAlchemyError as e:\n                error = str(e.__dict__[\"orig\"])\n                log.error(error)\n\n    def merge_tables(self, original_table: str, tmp_table: str, conn, file_id: int=None) -&gt; None:\n        \"\"\"\n        Merges data from a temporary table into the original table.\n\n        Args:\n            original_table (str): The name of the original table.\n            tmp_table (str): The name of the temporary table.\n        \"\"\"\n        metadata_obj = MetaData()\n        metadata_obj.reflect(bind=self.engine)\n        original_table_obj = metadata_obj.tables[str.lower(original_table)]\n        tmp_table_obj = metadata_obj.tables[str.lower(tmp_table)]\n        source_table_details = self._fetch_column_details(tmp_table)\n        target_table_details = self._fetch_column_details(original_table)\n        source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n\n        if file_id is not None:\n            delete_stmt = (\n                delete(original_table_obj)\n                .where(original_table_obj.c.file_meta_id == file_id)\n            )\n            res = conn.execute(delete_stmt)\n        common_columns = \", \".join(\n            set(source_table_details.keys())\n            &amp; set(target_table_details.keys())\n        )\n        insert_query = f\"\"\"\n        INSERT INTO {original_table} ({common_columns})\n        SELECT {common_columns}\n        FROM {tmp_table}\n        \"\"\"\n        result = conn.execute(text(insert_query))\n\n        log.info(\n            f\"Data inserted successfully, {result.rowcount} rows affected.\"\n        )\n\n    def close_connection(self) -&gt; None:\n        \"\"\"\n        Closes the database connection pool.\n        \"\"\"\n        self.engine.dispose()\n        log.info(\"Database connection pool has been closed.\")\n\n    def _fetch_column_details(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Fetches the details of columns in a specified table.\n\n        Args:\n            table_name (str): The name of the table to fetch details for.\n\n        Returns:\n            Dict[str, Any]: A dictionary mapping column names to their types.\n        \"\"\"\n        meta = MetaData()\n        table = Table(str.lower(table_name), meta, autoload_with=self.engine)\n        return {column.name: column.type for column in table.columns}\n\n    def _add_missing_columns(\n        self,\n        source_table_details: Dict[str, Any],\n        target_table: str,\n        target_table_details: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        Adds missing columns to a target table based on the source table's details.\n\n        Args:\n            source_table_details (Dict[str, Any]): Details of the source table's columns.\n            target_table (str): The name of the target table.\n            target_table_details (Dict[str, Any]): Details of the target table's columns.\n        \"\"\"\n        added_columns = []\n        source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n        target_table = str.lower(target_table)\n        with self.engine.connect() as conn:\n            for column, col_type in source_table_details.items():\n                if column not in target_table_details:\n                    alter_query = f\"ALTER TABLE {target_table} ADD COLUMN {column} {col_type}\"\n                    conn.execute(text(alter_query))\n                    log.info(\n                        f\"Added column {column} of type {col_type} to {target_table}\"\n                    )\n                    added_columns.append(column)\n        return added_columns\n\n    def _delete_columns(self, table_infos):\n        with self.engine.connect() as conn:\n            for table_info in table_infos:\n                table = table_info[0]\n                columns = table_info[-1]\n                for column in columns:\n                    alter_query = f\"ALTER TABLE {table} DROP COLUMN {column}\"\n                    try:\n                        conn.execute(text(alter_query))\n                        log.info(f\"Deleted column {column} in table {table}\")\n                    # except Exception as e:\n                    except SQLAlchemyError as e:\n                        log.error(f\"Error while deleting {column} from table {table}\")\n                        # print(type(e))\n\n    def update_table(self, table_name: str, df: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Updates a table in the database by replacing its content with a DataFrame.\n\n        Args:\n            table_name (str): The name of the table to update.\n            df (pd.DataFrame): The DataFrame containing the data to update.\n        \"\"\"\n        log.debug(\"Passed engine:\")\n        log.debug(self.engine)\n        try:\n            tmp_tbl = \"tmp_\" + str.lower(table_name)\n            with self.engine.connect() as conn:\n                df.to_sql(\n                    name=str.lower(table_name),\n                    con=conn,\n                    if_exists=\"replace\",\n                    index=False,\n                )\n                log.info(f\"Temporary table {tmp_tbl} created.\")\n\n        except Exception as err:\n            log.error(err)\n            raise\n\n    def _prepare_dataframe(self, data, data_column):\n        mapping = {col: ''.join(c for c in col if c.isalnum() or c == ' ' or c == '_') for col in data.columns}\n        data = data.rename(columns=mapping)\n\n        space_cols = [col for col in data.columns if ' ' in col]\n        mapping = {col: col.replace(' ', '_') for col in space_cols}\n        data = data.rename(columns=mapping)\n\n        cols=pd.Series(data.columns)\n\n        for dup in cols[cols.duplicated()].unique(): \n            cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n\n        # rename the columns with the cols list.\n        data.columns=cols\n        if data_column is not None:\n            if data_column in data.columns:\n                # data = data.rename(columns={data_column: 'timestamp'})\n                data[data_column] = pd.to_datetime(data[data_column]) # FIX TIMESTAMP setting\n                data.dropna(subset=[data_column], inplace=True)\n                data['timestamp'] = data[data_column] # FIX TIMESTAMP setting\n        return data\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.__init__","title":"<code>__init__(db_url, engine, config, file)</code>","text":"<p>Initializes the BaseLoader with the given database URL, engine, configuration, and FITS file.</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>The database URL.</p> required <code>engine</code> <code>Engine</code> <p>The SQLAlchemy engine for the database.</p> required <code>config</code> <code>ConfigType</code> <p>Configuration data for loading tables from the FITS file.</p> required <code>file</code> <code>FitsFile</code> <p>The FITS file object containing data to be loaded.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def __init__(\n    self, db_url: str, engine: engine, config: ConfigType, file: FitsFile\n):\n    \"\"\"\n    Initializes the BaseLoader with the given database URL, engine, configuration, and FITS file.\n\n    Args:\n        db_url (str): The database URL.\n        engine (engine.Engine): The SQLAlchemy engine for the database.\n        config (ConfigType): Configuration data for loading tables from the FITS file.\n        file (FitsFile): The FITS file object containing data to be loaded.\n    \"\"\"\n    self.db_url = db_url\n    self.engine = engine\n    self.config = config\n    self.file = file\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.check_table_exists","title":"<code>check_table_exists(table_name)</code>","text":"<p>Checks if a table exists in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def check_table_exists(self, table_name: str) -&gt; bool:\n    \"\"\"\n    Checks if a table exists in the database.\n\n    Args:\n        table_name (str): The name of the table to check.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n    with self.engine.connect() as conn:\n        query = text(\"SHOW TABLES LIKE :table_name\")\n        result = conn.execute(query, {\"table_name\": table_name})\n        return result.fetchone() is not None\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.clean_db","title":"<code>clean_db()</code>","text":"<p>Cleans the database by dropping specific tables and metadata tables.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def clean_db(self) -&gt; None:\n    \"\"\"\n    Cleans the database by dropping specific tables and metadata tables.\n    \"\"\"\n    with self.db_session() as session:\n    # Easyer way to drop tables\n        meta = MetaData()\n        meta.reflect(bind=self.engine)\n        for tbl in reversed(meta.sorted_tables):\n            tbl.drop(self.engine)\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.close_connection","title":"<code>close_connection()</code>","text":"<p>Closes the database connection pool.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def close_connection(self) -&gt; None:\n    \"\"\"\n    Closes the database connection pool.\n    \"\"\"\n    self.engine.dispose()\n    log.info(\"Database connection pool has been closed.\")\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.db_session","title":"<code>db_session()</code>","text":"<p>Creates and returns a new SQLAlchemy session for the database.</p> <p>Returns:</p> Name Type Description <code>Session</code> <code>Session</code> <p>A new SQLAlchemy session object.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def db_session(self) -&gt; Session:\n    \"\"\"\n    Creates and returns a new SQLAlchemy session for the database.\n\n    Returns:\n        Session: A new SQLAlchemy session object.\n    \"\"\"\n    Base.metadata.create_all(self.engine)\n    Session = sessionmaker(bind=self.engine)\n    return Session()\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.delete_meta_tables","title":"<code>delete_meta_tables(session)</code>","text":"<p>Drops FITS2DB created data tables from the database if they exist.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def delete_meta_tables(self, session: Session) -&gt; None:\n    \"\"\"\n    Drops FITS2DB created data tables from the database if they exist.\n    \"\"\"\n    log.debug(\"Start deletion of Meta tables\")\n    try:\n        session.query(Fits2DbTableMeta).delete()\n        log.debug(\"Run delete stmt for Fits2DbTableMeta\")\n        session.query(Fits2DbMeta).delete()\n        log.debug(\"Commit changes\")\n        session.commit()\n    except SQLAlchemyError as err:\n        log.error(err)\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.drop_table","title":"<code>drop_table(table_name)</code>","text":"<p>Drops a table from the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to drop.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table was successfully dropped, False otherwise.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def drop_table(self, table_name: str) -&gt; bool:\n    \"\"\"\n    Drops a table from the database.\n\n    Args:\n        table_name (str): The name of the table to drop.\n\n    Returns:\n        bool: True if the table was successfully dropped, False otherwise.\n    \"\"\"\n    with self.engine.connect() as conn:\n        transaction = conn.begin()  # Start a new transaction\n        try:\n            # Safely create the SQL string with the table name included\n            query = text(f\"DROP TABLE `{table_name}`\")\n            conn.execute(query)\n            transaction.commit()  # Commit the transaction if the drop is successful\n            return True\n        except Exception as e:\n            transaction.rollback()  # Roll back the transaction on error\n            print(f\"Failed to drop table {table_name}: {e}\")\n            return False\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.drop_user_tables","title":"<code>drop_user_tables(session)</code>","text":"<p>Drops FITS2DB created data tables from the database if they exist.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def drop_user_tables(self, session: Session) -&gt; None:\n    \"\"\"\n    Drops FITS2DB created data tables from the database if they exist.\n    \"\"\"\n    metadata = MetaData()\n    metadata.reflect(bind=self.engine)\n    log.info(metadata.tables)\n    try:\n        # db_table_names = self.get_tables(session)\n        # Bugfix that no tables were found\n        db_table_names = set(metadata.tables.keys())\n        for table_name in db_table_names:\n            if table_name in metadata.tables:\n                metadata.tables[table_name].drop(self.engine)\n                log.info(f\"Dropped table {table_name}\")\n            if table_name + \"_META\" in metadata.tables:\n                metadata.tables[table_name + \"_META\"].drop(self.engine)\n                log.info(f\"Dropped table {table_name+'_META'}\")\n            if \"TMP_\" + table_name in metadata.tables:\n                metadata.tables[\"TMP_\" + table_name].drop(self.engine)\n                log.info(f\"Dropped table {'TMP_'+table_name}\")\n\n    except SQLAlchemyError as e:\n        log.error(f\"An error occurred while dropping tables: {e}\")\n    finally:\n        self.engine.dispose()\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.execute_sql","title":"<code>execute_sql(sql)</code>","text":"<p>Executes a raw SQL query against the database.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query to execute.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def execute_sql(self, sql: str) -&gt; None:\n    \"\"\"\n    Executes a raw SQL query against the database.\n\n    Args:\n        sql (str): The SQL query to execute.\n    \"\"\"\n    with self.engine.connect() as conn:\n        try:\n            conn.execute(text(sql))\n            log.info(\"Query executed successfully\")\n        except SQLAlchemyError as e:\n            error = str(e.__dict__[\"orig\"])\n            log.error(error)\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.get_fits2db_meta","title":"<code>get_fits2db_meta()</code>","text":"<p>Retrieves the FITS2DB_META table from the database and returns it as a DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame containing the FITS2DB_META table data.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def get_fits2db_meta(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves the FITS2DB_META table from the database and returns it as a DataFrame.\n\n    Returns:\n        pd.DataFrame: The DataFrame containing the FITS2DB_META table data.\n    \"\"\"\n    try:\n        df = pd.read_sql_table(\"fits2db_meta\", con=self.engine)\n        return df\n    except Exception as err:\n        log.error(err)\n        raise\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.get_tables","title":"<code>get_tables(session)</code>","text":"<p>Retrieves and stores the names of all tables currently in the database.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def get_tables(self, session: Session) -&gt; set[str]:\n    \"\"\"\n    Retrieves and stores the names of all tables currently in the database.\n    \"\"\"\n    db_table_names = session.execute(\n        select(Fits2DbTableMeta.tablename)\n    ).fetchall()\n    db_table_names = [name[0] for name in db_table_names]\n    return set(db_table_names)\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.merge_tables","title":"<code>merge_tables(original_table, tmp_table, conn, file_id=None)</code>","text":"<p>Merges data from a temporary table into the original table.</p> <p>Parameters:</p> Name Type Description Default <code>original_table</code> <code>str</code> <p>The name of the original table.</p> required <code>tmp_table</code> <code>str</code> <p>The name of the temporary table.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def merge_tables(self, original_table: str, tmp_table: str, conn, file_id: int=None) -&gt; None:\n    \"\"\"\n    Merges data from a temporary table into the original table.\n\n    Args:\n        original_table (str): The name of the original table.\n        tmp_table (str): The name of the temporary table.\n    \"\"\"\n    metadata_obj = MetaData()\n    metadata_obj.reflect(bind=self.engine)\n    original_table_obj = metadata_obj.tables[str.lower(original_table)]\n    tmp_table_obj = metadata_obj.tables[str.lower(tmp_table)]\n    source_table_details = self._fetch_column_details(tmp_table)\n    target_table_details = self._fetch_column_details(original_table)\n    source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n\n    if file_id is not None:\n        delete_stmt = (\n            delete(original_table_obj)\n            .where(original_table_obj.c.file_meta_id == file_id)\n        )\n        res = conn.execute(delete_stmt)\n    common_columns = \", \".join(\n        set(source_table_details.keys())\n        &amp; set(target_table_details.keys())\n    )\n    insert_query = f\"\"\"\n    INSERT INTO {original_table} ({common_columns})\n    SELECT {common_columns}\n    FROM {tmp_table}\n    \"\"\"\n    result = conn.execute(text(insert_query))\n\n    log.info(\n        f\"Data inserted successfully, {result.rowcount} rows affected.\"\n    )\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.rename_table","title":"<code>rename_table(old_name, new_name)</code>","text":"<p>Renames a table in the database and adds an auto-incrementing primary key.</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> <p>The current name of the table.</p> required <code>new_name</code> <code>str</code> <p>The new name for the table.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def rename_table(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"\n    Renames a table in the database and adds an auto-incrementing primary key.\n\n    Args:\n        old_name (str): The current name of the table.\n        new_name (str): The new name for the table.\n    \"\"\"\n    with self.engine.connect() as conn:\n        try:\n            rename_stmt = text(f\"RENAME TABLE {old_name} TO {new_name}\")\n            id_stmt = text(f\"\"\"ALTER TABLE {new_name} \n                            ADD COLUMN id INT AUTO_INCREMENT,\n                            ADD PRIMARY KEY (id);\"\"\")\n            conn.execute(rename_stmt)\n            conn.execute(id_stmt)\n            log.info(\n                f\"Table renamed from {old_name} to {new_name} and added primamry key id.\"\n            )\n        except SQLAlchemyError as err:\n            log.error(err)\n            raise\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.update_file","title":"<code>update_file()</code>","text":"<p>Updates the metadata and data of the FITS file in the database.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def update_file(self) -&gt; None:\n    \"\"\"\n    Updates the metadata and data of the FITS file in the database.\n    \"\"\"\n    with self.db_session() as session:\n        file_record = self.update_fits2db_meta(session)\n        remaining_tables = self.get_current_file_tables(session, file_record)\n        # self.update_fits2db_table(session, file_record)\n        session.commit()\n        table_configs = self.config[\"fits_files\"][\"tables\"]\n        log.debug(\"Start upserting data\")\n        updated_tables = []\n        faulty_tables = []\n        new_tables = []\n        for table in table_configs:\n            log.debug(f\"Table in configs: {table}\")\n            table_name = table[\"name\"]\n            log.info(table_name)\n            log.info(table[\"ingest_all_columns\"])\n            try:\n                df = self.file.get_table(table_name)\n                table_name = str.lower(table_name)\n                df.data[\"FILE_META_ID\"] = file_record.id\n                df.data.columns = map(str.lower, df.data.columns)\n                df.meta.columns = map(str.lower, df.meta.columns)\n                date_column = table[\"date_column\"]\n                try:\n                    df.data = self._prepare_dataframe(df.data, date_column)\n                except ValueError as err:\n                    faulty_tables.append((table_name, date_column))\n                    continue\n\n                self.upsert_data_table(table_name, df.data, file_record.id)\n                remaining_tables.pop(table_name, None)\n                if self.check_table_exists(table_name):\n                    source_table_details = self._fetch_column_details('tmp_' + table_name)\n                    target_table_details = self._fetch_column_details(table_name)\n                    source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n                    new_columns = self._add_missing_columns(\n                        source_table_details, table_name, target_table_details\n                    )\n\n                    updated_tables.append((table_name, df, file_record.id, new_columns))\n                else:\n                    new_tables.append((table_name, df, file_record.id))\n                continue\n\n            except KeyError as err:\n                # log.error(f\"\\n {err}\")\n                log.warning(err.args[0])\n\n        if len(faulty_tables) &gt; 0:\n            log.error(f'Could not update File {self.file.file_path}')\n            for table_name, date_column in faulty_tables:\n                log.error(\n                    f'Error while parsing datetime column {date_column} in table {table_name}'\n                )\n            self._delete_columns(updated_tables)\n            session.commit()\n            for table, df, file_id, _ in updated_tables: \n                self.drop_table('tmp_' + table)\n            return\n\n        with self.engine.connect() as conn:\n            transaction = conn.begin()\n            try: \n                for table, df, file_id, new_columns in updated_tables: \n                    self.merge_tables(table, 'tmp_' + table, conn, file_id)\n                transaction.commit()\n            except Exception as e:\n                transaction.rollback()  # Rollback the transaction on error\n                self._delete_columns(updated_tables)\n                # TODO whatabout da file meta STUFF\n                log.error(f\"An error occurred: {e}\")\n                for table, df, file_id, _ in updated_tables: \n                    self.drop_table('tmp_' + table)\n                return\n        for table, df, file_id, _ in updated_tables: \n            self.drop_table('tmp_' + table)\n            self.write_table_meta(\n                table, df.data, session, file_record.id\n            )\n            self.update_table(table + \"_META\", df.meta)\n        for table, df, file_id in new_tables: \n            self.rename_table('tmp_' + table, table)\n            self.write_table_meta(\n                table, df.data, session, file_record.id\n            )\n            self.update_table(table + \"_META\", df.meta)\n\n        if self.config['fits_files']['delete_rows_from_missing_tables']:\n            for k, table in remaining_tables.items():\n                self.delete_file_from_table(session, file_record, table)\n\n        file_record.last_file_mutation = self.file.mdate\n        session.commit()\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.update_fits2db_table","title":"<code>update_fits2db_table(session, file_record)</code>","text":"<p>Delete entries from each table, that belong to a specific file</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>description</p> required <code>file_record</code> <code>Fits2DbMeta</code> <p>file meta data from the file whose data is to be deleted</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def update_fits2db_table(self, session: Session, file_record: Fits2DbMeta):\n    \"\"\" Delete entries from each table, that belong to a specific file\n\n    Args:\n        session (Session): _description_\n        file_record (Fits2DbMeta): file meta data from the file whose data is to be deleted\n    \"\"\"\n    tables_to_delete = session.query(Fits2DbTableMeta).filter(\n        Fits2DbTableMeta.file_meta_id == file_record.id\n    )\n    for table_meta in tables_to_delete:\n        tablename = table_meta.tablename\n        metadata = MetaData()\n        table = Table(tablename, metadata, autoload_with=self.engine)\n        delete_stmt = table.delete().where(\n            table.c.file_meta_id == file_record.id # change to lowercase\n        )\n        session.execute(delete_stmt)\n        log.info(\n            f\"Deleted rows in table '{tablename}' where file_meta_id = {file_record.id}\"\n        )\n\n    tables_to_delete.delete(synchronize_session=False)\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.update_table","title":"<code>update_table(table_name, df)</code>","text":"<p>Updates a table in the database by replacing its content with a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to update.</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the data to update.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def update_table(self, table_name: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Updates a table in the database by replacing its content with a DataFrame.\n\n    Args:\n        table_name (str): The name of the table to update.\n        df (pd.DataFrame): The DataFrame containing the data to update.\n    \"\"\"\n    log.debug(\"Passed engine:\")\n    log.debug(self.engine)\n    try:\n        tmp_tbl = \"tmp_\" + str.lower(table_name)\n        with self.engine.connect() as conn:\n            df.to_sql(\n                name=str.lower(table_name),\n                con=conn,\n                if_exists=\"replace\",\n                index=False,\n            )\n            log.info(f\"Temporary table {tmp_tbl} created.\")\n\n    except Exception as err:\n        log.error(err)\n        raise\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.upload_file","title":"<code>upload_file()</code>","text":"<p>Upserts the FITS file and its tables into the database.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def upload_file(self) -&gt; None:\n    \"\"\"\n    Upserts the FITS file and its tables into the database.\n    \"\"\"\n    with self.db_session() as session:\n        self.write_file_meta(session)\n        table_configs = self.config[\"fits_files\"][\"tables\"]\n        log.debug(\"Start upserting data\")\n\n        updated_tables = []\n        faulty_tables = []\n        new_tables = []\n\n        for table in table_configs:\n            log.debug(f\"Table in configs: {table}\")\n            table_name = table[\"name\"]\n            log.info(table_name)\n            log.info(table[\"ingest_all_columns\"])\n            try:\n                df = self.file.get_table(table_name)\n                table_name = str.lower(table_name)\n                df.data[\"FILE_META_ID\"] = self.new_file.id\n                df.data.columns = map(str.lower, df.data.columns) # change to lower\n                df.meta.columns = map(str.lower, df.meta.columns) # change to lower\n                date_column = table[\"date_column\"]\n                try:\n                    df.data = self._prepare_dataframe(df.data, date_column)\n                except ValueError:\n                    faulty_tables.append((table_name, date_column))\n                    continue\n\n                self.upsert_data_table(table_name, df.data)\n                if self.check_table_exists(table_name):\n                    source_table_details = self._fetch_column_details('tmp_' + table_name)\n                    target_table_details = self._fetch_column_details(table_name)\n                    source_table_details = {k.lower(): v for k, v in source_table_details.items()}\n                    new_columns = self._add_missing_columns(\n                        source_table_details, table_name, target_table_details\n                    )\n\n                    updated_tables.append((table_name, df, new_columns))\n                else:\n                    new_tables.append((table_name, df))\n                continue\n\n            except KeyError as err:\n                # log.error(f\"\\n {err}\")\n                log.warning(err.args[0])\n\n        if len(faulty_tables) &gt; 0:\n            log.error(f'Could not upload File {self.file.file_path}')\n            for table_name, date_column in faulty_tables:\n                log.error(\n                    f'Error while parsing datetime column {date_column} in table {table_name}'\n                )\n            self._delete_columns(updated_tables)\n            session.delete(self.new_file)\n            session.commit()\n            for table, df, new_columns in updated_tables: \n                self.drop_table('tmp_' + table)\n            return\n        with self.engine.connect() as conn:\n            transaction = conn.begin()\n            try: \n                for table, df, new_columns in updated_tables: \n                    self.merge_tables(table, 'tmp_' + table, conn)\n                transaction.commit()\n            except Exception as e:\n                transaction.rollback()  # Rollback the transaction on error\n                    # TODO delete file entry and exit function\n                self._delete_columns(updated_tables)\n                log.error(f'Could not Upload {self.file.file_path}')\n                log.error(f\"An error occurred: {e}\")\n                session.delete(self.new_file)\n                session.commit()\n                for table, df, new_columns in updated_tables: \n                    self.drop_table('tmp_' + table)\n                return\n        for table, df, new_columns in updated_tables: \n            self.drop_table('tmp_' + table)\n            self.update_table(str.lower(table) + \"_meta\", df.meta) # change to lower\n            self.write_table_meta(\n                table, df.data, session, self.new_file.id\n            )\n        for table, df in new_tables: \n            self.rename_table('tmp_' + table, table)\n            self.update_table(str.lower(table) + \"_meta\", df.meta) # change to lower\n            self.write_table_meta(\n                table, df.data, session, self.new_file.id\n            )\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.upsert_data_table","title":"<code>upsert_data_table(table_name, df, file_id=None)</code>","text":"<p>Upserts data into a table in the database. If the table exists, merges the data. Otherwise, renames the temporary table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to upsert.</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the data to upsert.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def upsert_data_table(self, table_name: str, df: pd.DataFrame, file_id: int=None) -&gt; None:\n    \"\"\"\n    Upserts data into a table in the database. If the table exists, merges the data.\n    Otherwise, renames the temporary table.\n\n    Args:\n        table_name (str): The name of the table to upsert.\n        df (pd.DataFrame): The DataFrame containing the data to upsert.\n    \"\"\"\n    log.debug(\"Passed engine:\")\n    log.debug(self.engine)\n\n    log.debug('clean df')  # TODO Extract\n\n\n    try:\n        tmp_tbl = \"tmp_\" + str.lower(table_name)  # change to lowercase\n        with self.engine.connect() as conn:\n            df.to_sql(\n                name=tmp_tbl,\n                con=conn,\n                if_exists=\"replace\",\n                index=False,\n            )\n            log.info(f\"Temporary table {tmp_tbl} created.\")\n\n        # if self.check_table_exists(table_name):\n            # self.merge_tables(table_name, tmp_tbl, file_id)\n            # self.drop_table(tmp_tbl)\n        # else:\n            # self.rename_table(tmp_tbl, table_name)\n\n    except Exception as err:\n        log.error(err)\n        raise\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.write_file_meta","title":"<code>write_file_meta(session)</code>","text":"<p>Writes metadata about the FITS file to the database.</p> Source code in <code>fits2db/adapters/base.py</code> <pre><code>def write_file_meta(self, session: Session) -&gt; None:\n    \"\"\"\n    Writes metadata about the FITS file to the database.\n    \"\"\"\n    log.debug(f\"Filepath {self.file.absolute_path.as_posix()}\")\n    self.new_file = Fits2DbMeta(\n        filename=self.file.file_name,\n        filepath=self.file.absolute_path.as_posix(),\n        last_file_mutation=self.file.mdate,\n    )\n    session.add(self.new_file)\n    session.commit()\n</code></pre>"},{"location":"reference/adapters_base/#fits2db.adapters.base.BaseLoader.write_table_meta","title":"<code>write_table_meta(tbl_name, df, session, file_id)</code>","text":"<p>Writes metadata about a table in the FITS file to the database.</p> <p>Parameters:</p> Name Type Description Default <code>tbl_name</code> <code>str</code> <p>The name of the table.</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame representing the table data.</p> required Source code in <code>fits2db/adapters/base.py</code> <pre><code>def write_table_meta(\n    self, tbl_name: str, df: pd.DataFrame, session: Session, file_id: int\n) -&gt; None:\n    \"\"\"\n    Writes metadata about a table in the FITS file to the database.\n\n    Args:\n        tbl_name (str): The name of the table.\n        df (pd.DataFrame): The DataFrame representing the table data.\n    \"\"\"\n    rows, cols = df.shape\n    table = session.execute(select(Fits2DbTableMeta).filter_by(file_meta_id=file_id, tablename=tbl_name)).scalar_one_or_none()\n    if table is None:\n        new_table = Fits2DbTableMeta(\n            file_meta_id=file_id,\n            tablename=str.lower(tbl_name),\n            record_count=rows,\n            column_count=cols,\n        )\n        session.add(new_table)\n    else:\n        table.record_count = rows\n        table.column_count = cols\n        table.tablename = str.lower(tbl_name)\n    session.commit()\n</code></pre>"},{"location":"reference/adapters_meta/","title":"ADAPTERS.META Module","text":"<p>Here are the Metadata table descriptions:</p> <p><pre><code>erDiagram\n    FITS2DB_META {\n        int id PK\n        varchar filename\n        varchar file_path\n        datetime file_last_mutated\n    }\n\n    FITS2DB_TABLE_META {\n        int id PK\n        varchar description\n        varchar tablename\n        int row_cnt\n        int col_cnt\n    }\n\n    YOUR_TABLE {\n        int id PK\n        varchar your_data\n        int metadata_id FK\n    }\n\n    YOUR_TABLE_META {\n        int id PK\n        text keyword\n        text value\n        int metadata_id FK\n    }\n\n    FITS2DB_META ||--|| FITS2DB_TABLE_META : \"foreign_id\"\n    FITS2DB_META ||--o| YOUR_TABLE : \"foreign_id\"\n    FITS2DB_TABLE_META ||--o| YOUR_TABLE_META : \"metadata_id\"</code></pre> This module defines the SQLAlchemy ORM models for the metadata tables used in the database. The models include:</p> <ul> <li>Fits2DbMeta: Represents metadata for FITS files.</li> <li>Fits2DbTableMeta: Represents metadata for tables related to FITS files.</li> </ul> <p>The relationships between these tables are visualized in the diagram above.</p>"},{"location":"reference/adapters_meta/#fits2db.adapters.meta.Fits2DbMeta","title":"<code>Fits2DbMeta</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy ORM model representing the FITS2DB_META table.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Primary key, auto-incremented.</p> <code>filename</code> <code>str</code> <p>Name of the FITS file.</p> <code>filepath</code> <code>str</code> <p>Path to the FITS file.</p> <code>last_db_update</code> <code>datetime</code> <p>Timestamp of the last database update.</p> <code>last_file_mutation</code> <code>datetime</code> <p>Timestamp of the last file modification.</p> <code>tables</code> <code>relationship</code> <p>Relationship to the Fits2DbTableMeta objects     associated with this file.</p> Source code in <code>fits2db/adapters/meta.py</code> <pre><code>class Fits2DbMeta(Base):\n    \"\"\"\n    SQLAlchemy ORM model representing the FITS2DB_META table.\n\n    Attributes:\n        id (int): Primary key, auto-incremented.\n        filename (str): Name of the FITS file.\n        filepath (str): Path to the FITS file.\n        last_db_update (datetime): Timestamp of the last database update.\n        last_file_mutation (datetime): Timestamp of the last file modification.\n        tables (relationship): Relationship to the Fits2DbTableMeta objects\n                associated with this file.\n\n    \"\"\"\n\n    __tablename__ = \"fits2db_meta\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    filename = Column(Text)\n    filepath = Column(Text)\n    last_db_update = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n        onupdate=datetime.now(timezone.utc),\n    )\n    last_file_mutation = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n    )\n    # Relationship to associate files with their tables\n    tables = relationship(\n        \"Fits2DbTableMeta\",\n        back_populates=\"file_meta\",\n        cascade=\"all, delete-orphan\",\n    )\n</code></pre>"},{"location":"reference/adapters_mysql/","title":"ADAPTERS.MYSQL Module","text":"<p>This module provides an interface for interacting with a MySQL database using SQLAlchemy. It includes methods for creating database connections, managing tables, and performing operations such as upsert and merge.</p> <p>Classes:</p> Name Description <code>MySQL</code> <p>Manages MySQL database operations related to FITS files.</p>"},{"location":"reference/adapters_mysql/#fits2db.adapters.mysql.MySQL","title":"<code>MySQL</code>","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Handles MySQL database operations for managing FITS files.</p> Inherits from <p>BaseLoader: A base class for loading data into databases.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>ConfigType</code> <p>Configuration details for database connection.</p> <code>engine</code> <code>ConfigType</code> <p>SQLAlchemy engine connected to the MySQL database.</p> Source code in <code>fits2db/adapters/mysql.py</code> <pre><code>class MySQL(BaseLoader):\n    \"\"\"\n    Handles MySQL database operations for managing FITS files.\n\n    Inherits from:\n        BaseLoader: A base class for loading data into databases.\n\n    Attributes:\n        config (ConfigType): Configuration details for database connection.\n        engine: SQLAlchemy engine connected to the MySQL database.\n    \"\"\"\n\n    def __init__(self, config: ConfigType, file: FitsFile) -&gt; None:\n        \"\"\"\n        Initializes the MySQL class with database configuration and a FITS file.\n\n        Args:\n            config (ConfigType): Configuration details for database connection.\n            file (FitsFile): FITS file to be processed.\n        \"\"\"\n        self.config = config\n        db_url = self.create_db_url()\n        engine = create_engine(db_url)\n        super().__init__(db_url, engine, config, file)\n\n    def create_db_url(self) -&gt; str:\n        \"\"\"\n        Creates a database connection URL from the configuration.\n\n        Returns:\n            str: Connection URL for MySQL.\n        \"\"\"\n        log.debug(\"Start create db url\")\n        user = self.config[\"database\"][\"user\"]\n        password = self.config[\"database\"][\"password\"]\n        host = self.config[\"database\"][\"host\"]\n        port = self.config[\"database\"][\"port\"]\n        db_name = self.config[\"database\"][\"db_name\"]\n        url = (\n            f\"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{db_name}\"\n        )\n        log.debug(\"Created url\")\n        log.info(url)\n        return url\n</code></pre>"},{"location":"reference/adapters_mysql/#fits2db.adapters.mysql.MySQL.__init__","title":"<code>__init__(config, file)</code>","text":"<p>Initializes the MySQL class with database configuration and a FITS file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigType</code> <p>Configuration details for database connection.</p> required <code>file</code> <code>FitsFile</code> <p>FITS file to be processed.</p> required Source code in <code>fits2db/adapters/mysql.py</code> <pre><code>def __init__(self, config: ConfigType, file: FitsFile) -&gt; None:\n    \"\"\"\n    Initializes the MySQL class with database configuration and a FITS file.\n\n    Args:\n        config (ConfigType): Configuration details for database connection.\n        file (FitsFile): FITS file to be processed.\n    \"\"\"\n    self.config = config\n    db_url = self.create_db_url()\n    engine = create_engine(db_url)\n    super().__init__(db_url, engine, config, file)\n</code></pre>"},{"location":"reference/adapters_mysql/#fits2db.adapters.mysql.MySQL.create_db_url","title":"<code>create_db_url()</code>","text":"<p>Creates a database connection URL from the configuration.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Connection URL for MySQL.</p> Source code in <code>fits2db/adapters/mysql.py</code> <pre><code>def create_db_url(self) -&gt; str:\n    \"\"\"\n    Creates a database connection URL from the configuration.\n\n    Returns:\n        str: Connection URL for MySQL.\n    \"\"\"\n    log.debug(\"Start create db url\")\n    user = self.config[\"database\"][\"user\"]\n    password = self.config[\"database\"][\"password\"]\n    host = self.config[\"database\"][\"host\"]\n    port = self.config[\"database\"][\"port\"]\n    db_name = self.config[\"database\"][\"db_name\"]\n    url = (\n        f\"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{db_name}\"\n    )\n    log.debug(\"Created url\")\n    log.info(url)\n    return url\n</code></pre>"},{"location":"reference/cli/","title":"CLI Module","text":""},{"location":"reference/cli/#fits2db.cli.helper_func.build","title":"<code>build(config_path, reset)</code>","text":"<p>Upsert all tables defnied in config.yml to databse</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=True))\n@click.option(\n    \"-r\",\n    \"--reset\",\n    default=True,\n    is_flag=True,\n    help=\"Rebuild entire database and drops old tables. If false it will error if there is already a able with the same name\",\n)\ndef build(config_path, reset):\n    \"\"\"Upsert all tables defnied in config.yml to databse\"\"\"\n    fits = Fits2db(config_path)\n    fits.build(reset)\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.helper_func.files","title":"<code>files(folder, config_path)</code>","text":"<p>Prints all files from given config.yaml file</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=True))\n@click.option(\n    \"-f\",\n    \"--folder\",\n    default=False,\n    is_flag=True,\n    help=\"Show all fits files in given folder\",\n)\ndef files(folder, config_path):\n    \"\"\"Prints all files from given config.yaml file\"\"\"\n    try:\n        if folder:\n            files = get_all_fits([config_path])\n\n        else:\n            fits = Fits2db(config_path)\n            files = fits.get_file_names()\n        for f in files:\n            click.echo(f)\n        click.echo(f\"Total of {len(files)} files\")\n    except Exception as err:\n        click.echo(err)\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.helper_func.init","title":"<code>init(config_path)</code>","text":"<p>Creates an example config file for you to change</p> <p>\b CONFIG_PATH     This argument can be a path to a folder or file.                 If you pass a file make sure to have the ending                 \".yml\" or \".yaml\" to get an valid config file</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=False))\ndef init(config_path):\n    \"\"\"Creates an example config file for you to change\n\n    \\b\n    CONFIG_PATH     This argument can be a path to a folder or file.\n                    If you pass a file make sure to have the ending\n                    \".yml\" or \".yaml\" to get an valid config file\n\n    \"\"\"\n    if generate_config(config_path):\n        click.echo(\"File generated sucessfull\")\n    else:\n        click.echo(\n            click.style(\"Failed to generate file\", blink=True, bold=True)\n        )\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.helper_func.tables","title":"<code>tables(config_path, matrix, csv, excel, filename)</code>","text":"<p>Prints all table names from all fits files from given config.yaml file</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=True))\n@click.option(\n    \"-m\",\n    \"--matrix\",\n    default=False,\n    is_flag=True,\n    help=\"Show all tables and files as matrix\",\n)\n@click.option(\n    \"--csv\", default=False, is_flag=True, help=\"Save the output as csv\"\n)\n@click.option(\n    \"--excel\", default=False, is_flag=True, help=\"Save the output as excel\"\n)\n@click.option(\n    \"--filename\",\n    default=\"output.csv\",\n    callback=validate_output_filename,\n    help=\"The filename for the output (required if --csv or --excel is specified).\",\n)\ndef tables(config_path, matrix, csv, excel, filename):\n    \"\"\"Prints all table names from all fits files from given config.yaml file\"\"\"\n    fits = Fits2db(config_path)\n    format = None\n    if csv:\n        format = \"csv\"\n    elif excel:\n        format = \"excel\"\n\n    if matrix:\n        m = fits.create_table_matrix(\n            output_format=format, output_file=filename\n        )\n        if format is None:\n            click.echo(m.to_string())\n    else:\n        names, _ = fits.get_table_names()\n        for f in names:\n            click.echo(f)\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.helper_func.update","title":"<code>update(config_path, force)</code>","text":"<p>Upsert all tables defnied in config.yml to database</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=True))\n@click.option(\n    \"-f\",\n    \"--force\",\n    default=False,\n    is_flag=True,\n    help=\"Force overwrite of files in config. Accepts skipping invalid files\",\n)\ndef update(config_path, force):\n    \"\"\"Upsert all tables defnied in config.yml to database\"\"\"\n    fits = Fits2db(config_path)\n    fits.update_db(force=force)\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.helper_func.upsert","title":"<code>upsert(config_path, force)</code>","text":"<p>Upsert all tables defnied in config.yml to databse</p> Source code in <code>fits2db/cli/helper_func.py</code> <pre><code>@click.command()\n@click.argument(\"config_path\", default=\".\", type=click.Path(exists=True))\n@click.option(\n    \"-f\",\n    \"--force\",\n    default=False,\n    is_flag=True,\n    help=\"Force overwrite of db. Accepts skipping invalid files\",\n)\ndef upsert(config_path, force):\n    \"\"\"Upsert all tables defnied in config.yml to databse\"\"\"\n    fits = Fits2db(config_path)\n    fits.upsert_to_db()\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.utils.set_verbosity","title":"<code>set_verbosity(ctx, param, value)</code>","text":"<p>Set verbosity of logs.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Context</code> <p>The Click context.</p> required <code>param</code> <code>Optional[Parameter]</code> <p>The Click parameter (unused).</p> required <code>value</code> <code>int</code> <p>The integer value of verbosity (number of <code>-v</code> flags).</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The verbosity value that was set.</p> Source code in <code>fits2db/cli/utils.py</code> <pre><code>def set_verbosity(\n    ctx: click.Context, param: Optional[click.Parameter], value: int\n) -&gt; int:\n    \"\"\"\n    Set verbosity of logs.\n\n    Args:\n        ctx (click.Context): The Click context.\n        param (Optional[click.Parameter]): The Click parameter (unused).\n        value (int): The integer value of verbosity (number of `-v` flags).\n\n    Returns:\n        int: The verbosity value that was set.\n    \"\"\"\n    levels = list(LOG_LEVELS.keys())\n    level = levels[min(len(levels) - 1, value)]\n    ctx.ensure_object(dict)\n    ctx.obj[\"logger\"] = configure_logger(level)\n    return value\n</code></pre>"},{"location":"reference/cli/#fits2db.cli.utils.validate_output_filename","title":"<code>validate_output_filename(ctx, param, value)</code>","text":"<p>Validate the output filename based on the selected file format options.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Context</code> <p>The Click context object.</p> required <code>param</code> <code>Optional[Parameter]</code> <p>The Click parameter object (unused).</p> required <code>value</code> <code>str</code> <p>The output filename provided by the user.</p> required <p>Raises:</p> Type Description <code>BadParameter</code> <p>If the filename does not meet the required conditions based on selected options.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The validated output filename.</p> Source code in <code>fits2db/cli/utils.py</code> <pre><code>def validate_output_filename(\n    ctx: click.Context, param: Optional[click.Parameter], value: str\n) -&gt; str:\n    \"\"\"\n    Validate the output filename based on the selected file format options.\n\n    Args:\n        ctx (click.Context): The Click context object.\n        param (Optional[click.Parameter]): The Click parameter object (unused).\n        value (str): The output filename provided by the user.\n\n    Raises:\n        click.BadParameter: If the filename does not meet the required conditions based on selected options.\n\n    Returns:\n        str: The validated output filename.\n    \"\"\"\n    if ctx.params.get(\"csv\") and not value.endswith(\".csv\"):\n        raise click.BadParameter(\"CSV filename must have a .csv extension.\")\n    if ctx.params.get(\"excel\") and not value.endswith(\".xlsx\"):\n        raise click.BadParameter(\"Excel filename must have a .xlsx extension.\")\n    if ctx.params.get(\"csv\") or ctx.params.get(\"excel\"):\n        if not value:\n            raise click.BadParameter(\n                \"Output filename is required when --csv or --excel is specified.\"\n            )\n    return value\n</code></pre>"},{"location":"reference/config/","title":"CONFIG Module","text":"<p>This module contains the configuration validation for the FITS to database application.</p> <p>This module contains the configuration validation for the FITS to database application.</p>"},{"location":"reference/config/#fits2db.config.config.generate_config","title":"<code>generate_config(path)</code>","text":"<p>Generate an example config file</p> Source code in <code>fits2db/config/config.py</code> <pre><code>def generate_config(path: Union[str, os.PathLike]) -&gt; bool:\n    \"\"\"Generate an example config file\"\"\"\n    try:\n        log.debug(f\"Passed path: {path}\")\n        if os.path.isdir(path):\n            log.debug(\"Path is a directory\")\n            file_path = os.path.join(path, \"config.yml\")\n            log.info(f\"Constructed config file path {file_path}\")\n        else:\n            if path.endswith(\".yml\") or path.endswith(\".yaml\"):\n                file_path = path\n            else:\n                raise ValueError(\n                    \"The path must either be a directory or specify a file ending with '.yml'\"\n                )\n\n        config_content = render_template(\n            \"config.yaml.j2\", {\"db_type\": \"mysql\"}\n        )\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(config_content)\n\n        log.info(f\"Configuration successfully written to {file_path}\")\n        return True\n\n    except Exception as e:\n        log.error(f\"Failed to write configuration file: {e}\")\n        return False\n</code></pre>"},{"location":"reference/config/#fits2db.config.config.get_configs","title":"<code>get_configs(path)</code>","text":"<p>Loads config file from given path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, PathLike]</code> <p>Path to config yaml file</p> required <p>Returns:</p> Name Type Description <code>ConfigType</code> <code>ConfigType</code> <p>Data from config file loaded and validated</p> Source code in <code>fits2db/config/config.py</code> <pre><code>def get_configs(path: Union[str, os.PathLike]) -&gt; ConfigType:\n    \"\"\"Loads config file from given path\n\n    Args:\n        path (Union[str, os.PathLike]): Path to config yaml file\n\n    Returns:\n        ConfigType: Data from config file loaded and validated\n    \"\"\"\n    config_validator = ConfigFileValidator(path=path)\n    with open(config_validator.path, \"r\", encoding=\"utf-8\") as file:\n        try:\n            config_data = yaml.safe_load(file)\n        except yaml.YAMLError as err:\n            log.error(\"YAML loading error: %s\", err)\n            raise\n            return {}\n\n    try:\n        data = ApplicationConfig(**config_data).model_dump()\n    except (TypeError, ValueError) as err:\n        log.error(\"Config file validation error: %s\", err)\n        raise\n        return {}\n\n    return data\n</code></pre>"},{"location":"reference/config/#fits2db.config.config.render_template","title":"<code>render_template(template_name, context)</code>","text":"<p>Renders a given template under the relative template directory</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>Name of template</p> required <code>context</code> <code>dict</code> <p>Dict with template conext</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered template as str</p> Source code in <code>fits2db/config/config.py</code> <pre><code>def render_template(template_name: str, context: dict) -&gt; str:\n    \"\"\"Renders a given template under the relative template directory\n\n    Args:\n        template_name (str): Name of template\n        context (dict): Dict with template conext\n\n    Returns:\n        str: Rendered template as str\n    \"\"\"\n    template_path = os.path.join(os.path.dirname(__file__), \"templates\")\n    log.info(f\"Template path: {template_path}\")\n    env = Environment(loader=FileSystemLoader(template_path))\n    template = env.get_template(template_name)\n    log.debug(f\"Template object: {template}\")\n    return template.render(context)\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.ApplicationConfig","title":"<code>ApplicationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Application configuration.</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>class ApplicationConfig(BaseModel):\n    \"\"\"Application configuration.\"\"\"\n\n    database: DatabaseConfig\n    fits_files: FitsConfig\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.ConfigFileValidator","title":"<code>ConfigFileValidator</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Validator if file exists.</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>class ConfigFileValidator(BaseModel):\n    \"\"\"Validator if file exists.\"\"\"\n\n    path: FilePath\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.DatabaseConfig","title":"<code>DatabaseConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Database configuration.</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>class DatabaseConfig(BaseModel):\n    \"\"\"Database configuration.\"\"\"\n\n    type: StrictStr\n    host: StrictStr\n    user: Optional[StrictStr] = None\n    password: Optional[StrictStr] = None\n    token: Optional[StrictStr] = None\n    port: Optional[int] = None\n    db_name: Optional[StrictStr] = None\n\n    @model_validator(mode=\"after\")\n    def validate_database(self) -&gt; Self:\n        \"\"\"Validate if supported db type\"\"\"\n        if self.type.lower() not in ACCEPTABLE_TYPES:\n            raise ValueError(f\"{self.type} is not a supported db\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def check_cedentials(self) -&gt; Self:\n        \"\"\"Validate credentials\"\"\"\n        user = self.user\n        password = self.password\n        token = self.token\n\n        if (not user or not password) and not token:\n            raise ValueError(\n                \"Either both user and password must be provided, or token must be provided.\"\n            )\n\n        if token and (user or password):\n            raise ValueError(\n                \"Cannot provide both user/password and token. Use one method for authentication.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.DatabaseConfig.check_cedentials","title":"<code>check_cedentials()</code>","text":"<p>Validate credentials</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_cedentials(self) -&gt; Self:\n    \"\"\"Validate credentials\"\"\"\n    user = self.user\n    password = self.password\n    token = self.token\n\n    if (not user or not password) and not token:\n        raise ValueError(\n            \"Either both user and password must be provided, or token must be provided.\"\n        )\n\n    if token and (user or password):\n        raise ValueError(\n            \"Cannot provide both user/password and token. Use one method for authentication.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.DatabaseConfig.validate_database","title":"<code>validate_database()</code>","text":"<p>Validate if supported db type</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_database(self) -&gt; Self:\n    \"\"\"Validate if supported db type\"\"\"\n    if self.type.lower() not in ACCEPTABLE_TYPES:\n        raise ValueError(f\"{self.type} is not a supported db\")\n    return self\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.FitsConfig","title":"<code>FitsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fits files configuraion.</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>class FitsConfig(BaseModel):\n    \"\"\"Fits files configuraion.\"\"\"\n\n    paths: list\n    tables: list[TableConfig]\n    delete_rows_from_missing_tables: Optional[bool] = False\n</code></pre>"},{"location":"reference/config/#fits2db.config.config_model.TableConfig","title":"<code>TableConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Table configuration.</p> Source code in <code>fits2db/config/config_model.py</code> <pre><code>class TableConfig(BaseModel):\n    \"\"\"Table configuration.\"\"\"\n\n    name: StrictStr\n    ingest_all_columns: Optional[bool] = True # No functionality yet\n    description: Optional[StrictStr] = None\n    columns: Optional[list] = None # No functionality yet\n    date_column: Optional[str] = None\n</code></pre>"},{"location":"reference/core/","title":"Module core","text":""},{"location":"reference/core/#fits2db.core.Fits2db","title":"<code>Fits2db</code>","text":"<p>A class to manage loading and interacting with FITS files in a SQL database.</p> <p>This class handles configuration, file discovery, and database operations related to FITS files, making it easy to integrate FITS data with a SQL database.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>class Fits2db:\n    \"\"\"\n    A class to manage loading and interacting with FITS files in a SQL database.\n\n    This class handles configuration, file discovery, and database operations\n    related to FITS files, making it easy to integrate FITS data with a SQL database.\n    \"\"\"\n\n    def __init__(self, config_path: str):\n        \"\"\"\n        Initialize the Fits2db class with a path to the configuration file.\n\n        Args:\n            config_path (str): Path to the configuration file.\n        \"\"\"\n        self.config_path = Path(config_path)\n        self.configs = get_configs(config_path)\n        self.fits_file_paths = self.get_file_names()\n\n    def get_file_names(self) -&gt; list[str]:\n        \"\"\"\n        Return a list of all absolute file paths found in the sources specified in the config file.\n\n        Returns:\n            List[str]: A list of absolute paths to the FITS files.\n        \"\"\"\n        paths = self.configs[\"fits_files\"][\"paths\"]\n        log.debug(f\"paths {paths}\")\n        log.info(\"run function\")\n        return list(dict.fromkeys(get_all_fits(paths)))\n\n    def get_file_infos(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate metadata for each FITS file, including filename, path, and last modification date.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing metadata for each FITS file.\n        \"\"\"\n        meta = []\n        for path in self.fits_file_paths:\n            path = Path(path)\n            absolute_path = path.resolve()\n            file_meta = {\n                \"filename\": path.name,\n                \"filepath\": absolute_path.as_posix(),\n                \"last_file_mutation\": datetime.fromtimestamp(\n                    os.path.getmtime(absolute_path)\n                ),\n            }\n            meta.append(file_meta)\n        df = pd.DataFrame(meta)\n        log.debug(df)\n        return df\n\n    def get_table_names(self) -&gt; Tuple[List[str], Dict[Path, List[str]]]:\n        \"\"\"\n        Retrieve table names from each FITS file.\n\n        Returns:\n            Tuple[List[str], Dict[Path, List[str]]]: A tuple containing,\n                a list of all unique table names and a dictionary mapping\n                file paths to their respective tables.\n        \"\"\"\n        self.all_table_names = []\n        self.file_table_dict = {}\n        for path in tqdm(self.fits_file_paths):\n            path = Path(path)\n            try:\n                file = FitsFile(path)\n                self.all_table_names.append(file.table_names)\n                self.file_table_dict[path] = file.table_names\n            except ValueError as err:\n                log.error(err)\n\n        self.all_table_names = flatten_and_deduplicate(self.all_table_names)\n        return self.all_table_names, self.file_table_dict\n\n    def create_table_matrix(\n        self,\n        output_format: Optional[str] = None,\n        output_file: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Create a matrix showing the presence of tables in each FITS file.\n\n        Args:\n            output_format (Optional[str]): The format in which to save the matrix ('csv' or 'excel').\n            output_file (Optional[str]): The name of the file to save the matrix.\n\n        Returns:\n            pd.DataFrame: A DataFrame showing which tables are present in which FITS files.\n        \"\"\"\n        all_table_names, file_table_dict = self.get_table_names()\n        file_names = [path.name for path in file_table_dict.keys()]\n        df = pd.DataFrame(index=file_names, columns=all_table_names)\n        for path, tables in file_table_dict.items():\n            file_name = path.name\n            for table in tables:\n                df.at[file_name, table] = \"X\"\n\n        df = df.fillna(\"\")\n\n        if output_format and file_name:\n            current_dir = os.getcwd()\n            full_file_path = os.path.join(current_dir, output_file)\n            if output_format.lower() == \"csv\":\n                df.to_csv(full_file_path)\n            elif output_format.lower() == \"excel\":\n                df.to_excel(full_file_path, index=True)\n\n        return df\n\n    def build(self, reset: bool = True) -&gt; None:\n        \"\"\"\n        Build the database from the FITS files, optionally resetting the database first.\n\n        Args:\n            reset (bool): Whether to reset the database before building.\n        \"\"\"\n        while True:\n            user_input = input(f\"This will remove all tables from the database '{self.configs['database']['db_name']}'.\\nDo you want to continue? (yes/no): \")\n            if user_input.lower() in [\"yes\", \"y\"]:\n                print(\"Continuing...\")\n                break\n            elif user_input.lower() in [\"no\", \"n\"]:\n                print(\"Exiting...\")\n                return\n            else:\n                print(\"Invalid input. Please enter yes/no.\")\n        log.debug(f\"Start building db with reset = {reset}\")\n        writer = DBWriter(self.configs)\n        if reset:\n            writer.clean_db()\n            log.debug(\"Clean db success start uploading files\")\n        for path in tqdm(self.fits_file_paths):\n            path = Path(path)\n            try:\n                file = FitsFile(path)\n                writer = DBWriter(self.configs, file)\n                writer.upsert()\n\n            except ValueError as err:\n                log.error(f\"\\n {err}\")\n\n    def get_db_diff(self, force=False) -&gt; None:\n        \"\"\"\n        Compare file metadata with database entries to find new or updated files.\n        \"\"\"\n        self.file_infos['last_file_mutation'] = self.file_infos['last_file_mutation'].dt.round('s')\n        merged_df = pd.merge(\n            self.file_infos,\n            self.db_file_infos,\n            on=[\"filename\", \"filepath\"],\n            how=\"left\",\n            suffixes=(\"_file\", \"_db\"),\n        )\n\n        new_files = merged_df[merged_df[\"last_file_mutation_db\"].isna()]\n        if force:\n            files2update = merged_df[merged_df[\"last_file_mutation_db\"].notna()]\n        else:\n            files2update = merged_df[\n                (\n                    merged_df[\"last_file_mutation_file\"]\n                    &gt; merged_df[\"last_file_mutation_db\"]\n                )\n            ]\n        self.new_files = new_files[\n            [\"filename\", \"filepath\", \"last_file_mutation_file\"]\n        ].rename(columns={\"last_file_mutation_file\": \"last_file_mutation\"})\n        self.files2update = files2update[\n            [\"filename\", \"filepath\", \"last_file_mutation_file\"]\n        ].rename(columns={\"last_file_mutation_file\": \"last_file_mutation\"})\n\n    def update_db(self, force=False) -&gt; None:\n        \"\"\"\n        Update the database with new or modified FITS files.\n        \"\"\"\n        self.file_infos = self.get_file_infos()\n        log.info(self.file_infos)\n        writer = DBWriter(self.configs)\n        self.db_file_infos = writer.get_db_file_infos()\n        log.info(self.db_file_infos)\n        self.get_db_diff(force=force) # TODO Make sideeffects of function clear!!\n\n        fits_file_paths = self.new_files[\"filepath\"].to_list()\n        for path in tqdm(fits_file_paths, desc=\"Upload new files\"):\n            path = Path(path)\n            try:\n                file = FitsFile(path)\n                writer = DBWriter(self.configs, file)\n                writer.upsert()\n\n            except ValueError as err:\n                log.error(f\"\\n {err}\")\n\n        fits_file_paths = self.files2update[\"filepath\"].to_list()\n        for path in tqdm(fits_file_paths, desc=\"Update files\"):\n            path = Path(path)\n            try:\n                file = FitsFile(path)\n                writer = DBWriter(self.configs, file)\n                writer.update()\n\n            except ValueError as err:\n                log.error(f\"\\n {err}\")\n\n    def upsert_to_db(self) -&gt; None:\n        \"\"\"\n        Insert or update all FITS files into the database, resetting the database first.\n        \"\"\"\n        log.debug(\"Start upsert to db\")\n        writer = DBWriter(self.configs)\n        writer.clean_db()\n        log.debug(\"Clean db success start uploading files\")\n        for path in tqdm(self.fits_file_paths):\n            path = Path(path)\n            try:\n                file = FitsFile(path)\n                writer = DBWriter(self.configs, file)\n                writer.upsert()\n\n            except ValueError as err:\n                log.error(f\"\\n {err}\")\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.__init__","title":"<code>__init__(config_path)</code>","text":"<p>Initialize the Fits2db class with a path to the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required Source code in <code>fits2db/core/core.py</code> <pre><code>def __init__(self, config_path: str):\n    \"\"\"\n    Initialize the Fits2db class with a path to the configuration file.\n\n    Args:\n        config_path (str): Path to the configuration file.\n    \"\"\"\n    self.config_path = Path(config_path)\n    self.configs = get_configs(config_path)\n    self.fits_file_paths = self.get_file_names()\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.build","title":"<code>build(reset=True)</code>","text":"<p>Build the database from the FITS files, optionally resetting the database first.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Whether to reset the database before building.</p> <code>True</code> Source code in <code>fits2db/core/core.py</code> <pre><code>def build(self, reset: bool = True) -&gt; None:\n    \"\"\"\n    Build the database from the FITS files, optionally resetting the database first.\n\n    Args:\n        reset (bool): Whether to reset the database before building.\n    \"\"\"\n    while True:\n        user_input = input(f\"This will remove all tables from the database '{self.configs['database']['db_name']}'.\\nDo you want to continue? (yes/no): \")\n        if user_input.lower() in [\"yes\", \"y\"]:\n            print(\"Continuing...\")\n            break\n        elif user_input.lower() in [\"no\", \"n\"]:\n            print(\"Exiting...\")\n            return\n        else:\n            print(\"Invalid input. Please enter yes/no.\")\n    log.debug(f\"Start building db with reset = {reset}\")\n    writer = DBWriter(self.configs)\n    if reset:\n        writer.clean_db()\n        log.debug(\"Clean db success start uploading files\")\n    for path in tqdm(self.fits_file_paths):\n        path = Path(path)\n        try:\n            file = FitsFile(path)\n            writer = DBWriter(self.configs, file)\n            writer.upsert()\n\n        except ValueError as err:\n            log.error(f\"\\n {err}\")\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.create_table_matrix","title":"<code>create_table_matrix(output_format=None, output_file=None)</code>","text":"<p>Create a matrix showing the presence of tables in each FITS file.</p> <p>Parameters:</p> Name Type Description Default <code>output_format</code> <code>Optional[str]</code> <p>The format in which to save the matrix ('csv' or 'excel').</p> <code>None</code> <code>output_file</code> <code>Optional[str]</code> <p>The name of the file to save the matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame showing which tables are present in which FITS files.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def create_table_matrix(\n    self,\n    output_format: Optional[str] = None,\n    output_file: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a matrix showing the presence of tables in each FITS file.\n\n    Args:\n        output_format (Optional[str]): The format in which to save the matrix ('csv' or 'excel').\n        output_file (Optional[str]): The name of the file to save the matrix.\n\n    Returns:\n        pd.DataFrame: A DataFrame showing which tables are present in which FITS files.\n    \"\"\"\n    all_table_names, file_table_dict = self.get_table_names()\n    file_names = [path.name for path in file_table_dict.keys()]\n    df = pd.DataFrame(index=file_names, columns=all_table_names)\n    for path, tables in file_table_dict.items():\n        file_name = path.name\n        for table in tables:\n            df.at[file_name, table] = \"X\"\n\n    df = df.fillna(\"\")\n\n    if output_format and file_name:\n        current_dir = os.getcwd()\n        full_file_path = os.path.join(current_dir, output_file)\n        if output_format.lower() == \"csv\":\n            df.to_csv(full_file_path)\n        elif output_format.lower() == \"excel\":\n            df.to_excel(full_file_path, index=True)\n\n    return df\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.get_db_diff","title":"<code>get_db_diff(force=False)</code>","text":"<p>Compare file metadata with database entries to find new or updated files.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def get_db_diff(self, force=False) -&gt; None:\n    \"\"\"\n    Compare file metadata with database entries to find new or updated files.\n    \"\"\"\n    self.file_infos['last_file_mutation'] = self.file_infos['last_file_mutation'].dt.round('s')\n    merged_df = pd.merge(\n        self.file_infos,\n        self.db_file_infos,\n        on=[\"filename\", \"filepath\"],\n        how=\"left\",\n        suffixes=(\"_file\", \"_db\"),\n    )\n\n    new_files = merged_df[merged_df[\"last_file_mutation_db\"].isna()]\n    if force:\n        files2update = merged_df[merged_df[\"last_file_mutation_db\"].notna()]\n    else:\n        files2update = merged_df[\n            (\n                merged_df[\"last_file_mutation_file\"]\n                &gt; merged_df[\"last_file_mutation_db\"]\n            )\n        ]\n    self.new_files = new_files[\n        [\"filename\", \"filepath\", \"last_file_mutation_file\"]\n    ].rename(columns={\"last_file_mutation_file\": \"last_file_mutation\"})\n    self.files2update = files2update[\n        [\"filename\", \"filepath\", \"last_file_mutation_file\"]\n    ].rename(columns={\"last_file_mutation_file\": \"last_file_mutation\"})\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.get_file_infos","title":"<code>get_file_infos()</code>","text":"<p>Generate metadata for each FITS file, including filename, path, and last modification date.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing metadata for each FITS file.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def get_file_infos(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate metadata for each FITS file, including filename, path, and last modification date.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing metadata for each FITS file.\n    \"\"\"\n    meta = []\n    for path in self.fits_file_paths:\n        path = Path(path)\n        absolute_path = path.resolve()\n        file_meta = {\n            \"filename\": path.name,\n            \"filepath\": absolute_path.as_posix(),\n            \"last_file_mutation\": datetime.fromtimestamp(\n                os.path.getmtime(absolute_path)\n            ),\n        }\n        meta.append(file_meta)\n    df = pd.DataFrame(meta)\n    log.debug(df)\n    return df\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.get_file_names","title":"<code>get_file_names()</code>","text":"<p>Return a list of all absolute file paths found in the sources specified in the config file.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of absolute paths to the FITS files.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def get_file_names(self) -&gt; list[str]:\n    \"\"\"\n    Return a list of all absolute file paths found in the sources specified in the config file.\n\n    Returns:\n        List[str]: A list of absolute paths to the FITS files.\n    \"\"\"\n    paths = self.configs[\"fits_files\"][\"paths\"]\n    log.debug(f\"paths {paths}\")\n    log.info(\"run function\")\n    return list(dict.fromkeys(get_all_fits(paths)))\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.get_table_names","title":"<code>get_table_names()</code>","text":"<p>Retrieve table names from each FITS file.</p> <p>Returns:</p> Type Description <code>Tuple[List[str], Dict[Path, List[str]]]</code> <p>Tuple[List[str], Dict[Path, List[str]]]: A tuple containing, a list of all unique table names and a dictionary mapping file paths to their respective tables.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def get_table_names(self) -&gt; Tuple[List[str], Dict[Path, List[str]]]:\n    \"\"\"\n    Retrieve table names from each FITS file.\n\n    Returns:\n        Tuple[List[str], Dict[Path, List[str]]]: A tuple containing,\n            a list of all unique table names and a dictionary mapping\n            file paths to their respective tables.\n    \"\"\"\n    self.all_table_names = []\n    self.file_table_dict = {}\n    for path in tqdm(self.fits_file_paths):\n        path = Path(path)\n        try:\n            file = FitsFile(path)\n            self.all_table_names.append(file.table_names)\n            self.file_table_dict[path] = file.table_names\n        except ValueError as err:\n            log.error(err)\n\n    self.all_table_names = flatten_and_deduplicate(self.all_table_names)\n    return self.all_table_names, self.file_table_dict\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.update_db","title":"<code>update_db(force=False)</code>","text":"<p>Update the database with new or modified FITS files.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def update_db(self, force=False) -&gt; None:\n    \"\"\"\n    Update the database with new or modified FITS files.\n    \"\"\"\n    self.file_infos = self.get_file_infos()\n    log.info(self.file_infos)\n    writer = DBWriter(self.configs)\n    self.db_file_infos = writer.get_db_file_infos()\n    log.info(self.db_file_infos)\n    self.get_db_diff(force=force) # TODO Make sideeffects of function clear!!\n\n    fits_file_paths = self.new_files[\"filepath\"].to_list()\n    for path in tqdm(fits_file_paths, desc=\"Upload new files\"):\n        path = Path(path)\n        try:\n            file = FitsFile(path)\n            writer = DBWriter(self.configs, file)\n            writer.upsert()\n\n        except ValueError as err:\n            log.error(f\"\\n {err}\")\n\n    fits_file_paths = self.files2update[\"filepath\"].to_list()\n    for path in tqdm(fits_file_paths, desc=\"Update files\"):\n        path = Path(path)\n        try:\n            file = FitsFile(path)\n            writer = DBWriter(self.configs, file)\n            writer.update()\n\n        except ValueError as err:\n            log.error(f\"\\n {err}\")\n</code></pre>"},{"location":"reference/core/#fits2db.core.Fits2db.upsert_to_db","title":"<code>upsert_to_db()</code>","text":"<p>Insert or update all FITS files into the database, resetting the database first.</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def upsert_to_db(self) -&gt; None:\n    \"\"\"\n    Insert or update all FITS files into the database, resetting the database first.\n    \"\"\"\n    log.debug(\"Start upsert to db\")\n    writer = DBWriter(self.configs)\n    writer.clean_db()\n    log.debug(\"Clean db success start uploading files\")\n    for path in tqdm(self.fits_file_paths):\n        path = Path(path)\n        try:\n            file = FitsFile(path)\n            writer = DBWriter(self.configs, file)\n            writer.upsert()\n\n        except ValueError as err:\n            log.error(f\"\\n {err}\")\n</code></pre>"},{"location":"reference/core/#fits2db.core.get_all_fits","title":"<code>get_all_fits(paths)</code>","text":"<p>Searches recursive throught all folders of given list of paths for fits files,  and gives them back. Args:     paths (list): A list of paths to search recursivly for fits files.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Returns list of absolute paths of all fits files</p> Source code in <code>fits2db/core/core.py</code> <pre><code>def get_all_fits(paths: list) -&gt; list:\n    \"\"\"Searches recursive throught all folders of given list of paths for fits files,\n     and gives them back.\n    Args:\n        paths (list): A list of paths to search recursivly for fits files.\n\n    Returns:\n        list: Returns list of absolute paths of all fits files\n    \"\"\"\n    all_fits_files = []\n    for path in paths:\n        if os.path.isdir(path):\n            for root, _, files in os.walk(path):\n                for file in files:\n                    if file.endswith(\".fits\"):\n                        all_fits_files.append(os.path.join(root, file))\n        elif os.path.isfile(path) and path.endswith(\".fits\"):\n            all_fits_files.append(path)\n    return all_fits_files\n</code></pre>"},{"location":"reference/database/","title":"Database Reference","text":"<p>When we upload data to the a Database the main idea is to have a mantainable stream of data into our database. For this fits2db creates some helper tables that keeps track of the files that are ingested. </p> <p><code>FITS2DB_META</code> contains the the meta data of each file that is loaded into the database.</p> <p>Here are the Metadata table descriptions:</p> <p><pre><code>erDiagram\n    FITS2DB_META {\n        int id PK\n        varchar filename\n        varchar file_path\n        datetime file_last_mutated\n    }\n\n    FITS2DB_TABLE_META {\n        int id PK\n        varchar description\n        varchar tablename\n        int row_cnt\n        int col_cnt\n    }\n\n    YOUR_TABLE {\n        int id PK\n        varchar your_data\n        int metadata_id FK\n    }\n\n    YOUR_TABLE_META {\n        int id PK\n        text keyword\n        text value\n        int metadata_id FK\n    }\n\n    FITS2DB_META ||--|| FITS2DB_TABLE_META : \"foreign_id\"\n    FITS2DB_META ||--o| YOUR_TABLE : \"foreign_id\"\n    FITS2DB_TABLE_META ||--o| YOUR_TABLE_META : \"metadata_id\"</code></pre> This module defines the SQLAlchemy ORM models for the metadata tables used in the database. The models include:</p> <ul> <li>Fits2DbMeta: Represents metadata for FITS files.</li> <li>Fits2DbTableMeta: Represents metadata for tables related to FITS files.</li> </ul> <p>The relationships between these tables are visualized in the diagram above.</p>"},{"location":"reference/database/#fits2db.adapters.meta.Fits2DbMeta","title":"<code>Fits2DbMeta</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy ORM model representing the FITS2DB_META table.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Primary key, auto-incremented.</p> <code>filename</code> <code>str</code> <p>Name of the FITS file.</p> <code>filepath</code> <code>str</code> <p>Path to the FITS file.</p> <code>last_db_update</code> <code>datetime</code> <p>Timestamp of the last database update.</p> <code>last_file_mutation</code> <code>datetime</code> <p>Timestamp of the last file modification.</p> <code>tables</code> <code>relationship</code> <p>Relationship to the Fits2DbTableMeta objects     associated with this file.</p> Source code in <code>fits2db/adapters/meta.py</code> <pre><code>class Fits2DbMeta(Base):\n    \"\"\"\n    SQLAlchemy ORM model representing the FITS2DB_META table.\n\n    Attributes:\n        id (int): Primary key, auto-incremented.\n        filename (str): Name of the FITS file.\n        filepath (str): Path to the FITS file.\n        last_db_update (datetime): Timestamp of the last database update.\n        last_file_mutation (datetime): Timestamp of the last file modification.\n        tables (relationship): Relationship to the Fits2DbTableMeta objects\n                associated with this file.\n\n    \"\"\"\n\n    __tablename__ = \"fits2db_meta\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    filename = Column(Text)\n    filepath = Column(Text)\n    last_db_update = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n        onupdate=datetime.now(timezone.utc),\n    )\n    last_file_mutation = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n    )\n    # Relationship to associate files with their tables\n    tables = relationship(\n        \"Fits2DbTableMeta\",\n        back_populates=\"file_meta\",\n        cascade=\"all, delete-orphan\",\n    )\n</code></pre>"},{"location":"reference/fits/","title":"FITS Module","text":""},{"location":"reference/fits/#fits2db.fits.FitsFile","title":"<code>FitsFile</code>  <code>dataclass</code>","text":"Source code in <code>fits2db/fits/fits.py</code> <pre><code>@dataclass\nclass FitsFile:\n    file_path: Path\n    absolute_path: Path = field(init=False)\n    file_name: str = field(init=False)\n    file_size: int = field(init=False)\n    hdul: fits.HDUList = field(init=False)\n    table_names: List = field(init=False)\n    config: dict = field(init=False)\n\n    def __post_init__(self):\n        self.check_path()\n        self.load_file()\n        self.get_table_names()\n        self.mtime = time.ctime(os.path.getmtime(self.absolute_path))\n        self.mdate = datetime.fromtimestamp(\n            os.path.getmtime(self.absolute_path)\n        )\n\n    def check_path(self):\n        if not self.file_path.exists():\n            raise FileNotFoundError(\n                f\"The file {self.file_path} does not exist.\"\n            )\n        if not self.file_path.is_file():\n            raise ValueError(f\"The path {self.file_path} is not a file.\")\n\n        self.absolute_path = self.file_path.resolve()\n        self.file_name = self.file_path.name\n        self.file_size = self.file_path.stat().st_size\n\n    def load_file(self):\n        if self.file_path.suffix.lower() != \".fits\":\n            raise ValueError(f\"The file {self.file_path} is not a FITS file.\")\n        try:\n            self.hdul = fits.open(self.absolute_path, memmap=True)\n        except Exception as e:\n            raise ValueError(\n                f\"The file {self.file_path} is not a valid FITS file: {e}\"\n            )\n\n    def get_table(self, name: str) -&gt; FitsTable:\n        \"\"\"Access a specific table by index without loading all tables into memory.\"\"\"\n        if name not in self.table_names:\n            raise KeyError(\n                f\"\\n Key {name} is not a table in HDUL. \\n in file {self.absolute_path}\"\n            )\n        hdu = self.hdul[name]\n        data = self.extract_data(hdu)\n        meta = self.extract_meta(hdu)\n        fits_table = FitsTable(name=name, data=data, meta=meta)\n        return fits_table\n\n    def get_table_names(self):\n        \"\"\"Return the names of all tables in the FITS file.\"\"\"\n        self.table_names = [\n            hdu.name\n            for hdu in self.hdul\n            if isinstance(hdu, (fits.BinTableHDU, fits.TableHDU))\n        ]\n\n    def close(self):\n        \"\"\"Close the FITS file.\"\"\"\n        if hasattr(self, \"hdul\") and self.hdul:\n            self.hdul.close()\n            self.hdul = None\n\n    def __del__(self):\n        \"\"\"Ensure resources are freed when the object is deleted.\"\"\"\n        self.close()\n\n    def extract_data(self, hdu: fits.Card) -&gt; pd.DataFrame:\n        data = hdu.data\n        # Convert all big-endian columns to little-endian\n        columns = data.columns.names\n        little_endian_data = {\n            col: data[col].byteswap().newbyteorder()\n            if data[col].dtype.byteorder == \"&gt;\"\n            else data[col]\n            for col in columns\n        }\n        df = pd.DataFrame(little_endian_data)\n        return df\n\n    def extract_meta(self, hdu: fits.Card) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            list(hdu.header.items()), columns=[\"Keyword\", \"Value\"]\n        )\n</code></pre>"},{"location":"reference/fits/#fits2db.fits.FitsFile.__del__","title":"<code>__del__()</code>","text":"<p>Ensure resources are freed when the object is deleted.</p> Source code in <code>fits2db/fits/fits.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure resources are freed when the object is deleted.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/fits/#fits2db.fits.FitsFile.close","title":"<code>close()</code>","text":"<p>Close the FITS file.</p> Source code in <code>fits2db/fits/fits.py</code> <pre><code>def close(self):\n    \"\"\"Close the FITS file.\"\"\"\n    if hasattr(self, \"hdul\") and self.hdul:\n        self.hdul.close()\n        self.hdul = None\n</code></pre>"},{"location":"reference/fits/#fits2db.fits.FitsFile.get_table","title":"<code>get_table(name)</code>","text":"<p>Access a specific table by index without loading all tables into memory.</p> Source code in <code>fits2db/fits/fits.py</code> <pre><code>def get_table(self, name: str) -&gt; FitsTable:\n    \"\"\"Access a specific table by index without loading all tables into memory.\"\"\"\n    if name not in self.table_names:\n        raise KeyError(\n            f\"\\n Key {name} is not a table in HDUL. \\n in file {self.absolute_path}\"\n        )\n    hdu = self.hdul[name]\n    data = self.extract_data(hdu)\n    meta = self.extract_meta(hdu)\n    fits_table = FitsTable(name=name, data=data, meta=meta)\n    return fits_table\n</code></pre>"},{"location":"reference/fits/#fits2db.fits.FitsFile.get_table_names","title":"<code>get_table_names()</code>","text":"<p>Return the names of all tables in the FITS file.</p> Source code in <code>fits2db/fits/fits.py</code> <pre><code>def get_table_names(self):\n    \"\"\"Return the names of all tables in the FITS file.\"\"\"\n    self.table_names = [\n        hdu.name\n        for hdu in self.hdul\n        if isinstance(hdu, (fits.BinTableHDU, fits.TableHDU))\n    ]\n</code></pre>"},{"location":"reference/log/","title":"LOG Module","text":"<p>Logging configuration module for Fits2db.</p> <p>This module sets up and configures logging for the Fits2db package, supporting various log levels and formats. Logs can be directed to both the console and an optional debug file.</p> <p>Example usage</p> <p>entry point of programm<pre><code>from .log import configure_logger\n\nlevel = \"DEBUG\"\nlog = configure_logger(level)\nlog.info(\"Some Message\")\n</code></pre> Usage in other modules<pre><code>level = \"DEBUG\"\nconfigure_logger(level)\n</code></pre></p>"},{"location":"reference/log/#fits2db.log.configure_logger","title":"<code>configure_logger(level='DEBUG', debug_file=None)</code>","text":"<p>Configure logging for python_package.</p> <p>Set up logging to stdout with given level. If <code>debug_file</code> is given set up logging to file with DEBUG level.</p> Source code in <code>fits2db/log.py</code> <pre><code>def configure_logger(\n    level: str = \"DEBUG\", debug_file: str | None = None\n) -&gt; logging.Logger:\n    \"\"\"Configure logging for python_package.\n\n    Set up logging to stdout with given level. If ``debug_file`` is given set\n    up logging to file with DEBUG level.\n    \"\"\"\n    logger = logging.getLogger(\"fits2db\")\n    logger.setLevel(logging.DEBUG)\n\n    # Remove all attached handlers, in case there was\n    # a logger with using the name 'fits2db'\n    del logger.handlers[:]\n\n    # Create a file handler if a log file is provided\n    if debug_file is not None:\n        debug_formatter = logging.Formatter(\n            LOG_FORMATS[\"DEBUG\"], datefmt=DATE_FORMAT\n        )\n        file_handler = logging.FileHandler(debug_file)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(debug_formatter)\n        logger.addHandler(file_handler)\n\n    log_level = LOG_LEVELS.get(level, logging.DEBUG)\n    log_formatter = logging.Formatter(\n        LOG_FORMATS.get(level, LOG_FORMATS[\"DEBUG\"]), datefmt=DATE_FORMAT\n    )\n\n    # Create a stream handler\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setLevel(log_level)\n    stream_handler.setFormatter(log_formatter)\n    logger.addHandler(stream_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/reference/","title":"API Reference","text":""},{"location":"reference/reference/#fits2db","title":"fits2db","text":"<p>Fits2DB CLI can be used to extract data from fits files and load them into a Database. For this, the CLI has various helper functions to inspect the content of fits files and run some checks to see if the expected content is available.</p> Tips <ul> <li>Check files before loading them into the database to have fewer worries once loaded.</li> <li>You can also set a fail flag to fail the ingestion if some columns or data points are missing.</li> </ul> Example Usage <pre><code>fits2db upsert path/to/your/config.yaml\n</code></pre> <p>Usage:</p> <pre><code>fits2db [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --verbosity  Increase verbosity of the log output. Use -v for WARNING,\n                   -vv for INFO, -vvv for DEBUG.\n  --version        Show the version and exit.\n  --help           Show this message and exit.\n</code></pre>"},{"location":"reference/reference/#fits2db-build","title":"fits2db build","text":"<p>Upsert all tables defnied in config.yml to databse</p> <p>Usage:</p> <pre><code>fits2db build [OPTIONS] [CONFIG_PATH]\n</code></pre> <p>Options:</p> <pre><code>  -r, --reset  Rebuild entire database and drops old tables. If false it will\n               error if there is already a able with the same name\n  --help       Show this message and exit.\n</code></pre>"},{"location":"reference/reference/#fits2db-files","title":"fits2db files","text":"<p>Prints all files from given config.yaml file</p> <p>Usage:</p> <pre><code>fits2db files [OPTIONS] [CONFIG_PATH]\n</code></pre> <p>Options:</p> <pre><code>  -f, --folder  Show all fits files in given folder\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/reference/#fits2db-init","title":"fits2db init","text":"<p>Creates an example config file for you to change</p> <p>CONFIG_PATH     This argument can be a path to a folder or file.                 If you pass a file make sure to have the ending                 \".yml\" or \".yaml\" to get an valid config file</p> <p>Usage:</p> <pre><code>fits2db init [OPTIONS] [CONFIG_PATH]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/reference/#fits2db-tables","title":"fits2db tables","text":"<p>Prints all table names from all fits files from given config.yaml file</p> <p>Usage:</p> <pre><code>fits2db tables [OPTIONS] [CONFIG_PATH]\n</code></pre> <p>Options:</p> <pre><code>  -m, --matrix     Show all tables and files as matrix\n  --csv            Save the output as csv\n  --excel          Save the output as excel\n  --filename TEXT  The filename for the output (required if --csv or --excel\n                   is specified).\n  --help           Show this message and exit.\n</code></pre>"},{"location":"reference/reference/#fits2db-update","title":"fits2db update","text":"<p>Upsert all tables defnied in config.yml to database</p> <p>Usage:</p> <pre><code>fits2db update [OPTIONS] [CONFIG_PATH]\n</code></pre> <p>Options:</p> <pre><code>  -f, --force  Force overwrite of files in config. Accepts skipping invalid\n               files\n  --help       Show this message and exit.\n</code></pre>"},{"location":"user-guide/about-us/","title":"About fits2db","text":"<p>Do you have a large number of daily generated FITS files that you need to manage within a complex folder structure? If so, this project might be exactly what you're looking for! The primary goal of <code>fits2db</code> is to provide a straightforward and efficient way to load the data contained in your FITS files into a structured SQL database, making it easier to manage, query, and analyze.</p>"},{"location":"user-guide/about-us/#why-should-i-use-a-database","title":"Why should I use a database?","text":"<p>Managing a large collection of FITS files directly in a folder structure can become cumbersome and inefficient as the number of files grows. Here\u2019s why using a database is a better approach:</p> <ul> <li> <p>Centralized Data Management: A database allows you to centralize your data, making it easier to access, search, and manage compared to navigating through a multitude of files spread across directories.</p> </li> <li> <p>Efficient Querying: SQL databases are optimized for querying. You can quickly search, filter, and retrieve specific data from large datasets, which would be far more time-consuming if you were parsing through individual files manually.</p> </li> <li> <p>Data Integrity and Consistency: Using a database ensures that your data remains consistent and avoids issues such as duplicate entries or data loss, which can happen when managing files individually.</p> </li> <li> <p>Scalability: As your dataset grows, managing thousands or even millions of files in a folder structure can become impractical. Databases are designed to scale efficiently, allowing you to handle larger volumes of data without performance degradation.</p> </li> <li> <p>Automation: With fits2db, you can automate the process of loading new data into your database, saving you time and reducing the likelihood of human error.</p> </li> </ul>"},{"location":"user-guide/about-us/#who-should-use-fits2db","title":"Who Should Use fits2db?","text":"<p><code>fits2db</code> is ideal for researchers, data scientists, and engineers who deal with large volumes of FITS files on a regular basis and need an efficient way to organize, manage, and query their data. This tool is particularly useful in fields like astronomy, where FITS files are commonly used to store observations and measurement data.</p> <p>If you are looking to streamline your data management process, minimize manual file handling, and take advantage of the querying power of SQL, fits2db is a great fit for your workflow.</p>"},{"location":"user-guide/about-us/#probably-not-for-you","title":"Probably Not for You","text":"<p>On the other hand, if your needs are simple\u2014such as opening a single FITS file occasionally to extract data <code>fits2db</code> might not be the right tool. In such cases, a lightweight FITS file viewer or a scripting approach with <code>Astropy</code> to handle individual files may be more appropriate.</p>"},{"location":"user-guide/about-us/#how-does-fits2db-work","title":"How Does fits2db Work?","text":"<p>fits2db is designed to streamline the process of managing and updating large datasets stored in FITS files by integrating them into a SQL database. Here's a high-level overview of the workflow:</p> <ul> <li> <p>Configuration Setup: You start by creating a configuration file that outlines the data sources (the directories containing your FITS files) and the destination (your SQL database). This configuration serves as the blueprint for how fits2db interacts with your data.</p> </li> <li> <p>Building the Database: Using the configuration file, fits2db initializes the database by creating tables that correspond to the data structures in your FITS files. It then populates these tables with the data, ensuring that everything is organized and ready for querying.</p> </li> <li> <p>Updating the Database: As your data evolves\u2014new FITS files are added, or existing ones are modified\u2014fits2db allows you to update your database efficiently. It identifies changes and ensures that only new or updated data is uploaded, keeping your database synchronized with the latest information.</p> </li> <li> <p>Metadata Management: Throughout the process, fits2db automatically maintains metadata tables that track the files, tables, and data within your database. This metadata is crucial for ensuring data integrity and facilitating easy management of your data over time.</p> </li> <li> <p>Lifecycle Management: If needed, fits2db can also clean up the database by removing the entire table structure, giving you control over the full lifecycle of your data from creation to deletion.</p> </li> </ul>"},{"location":"user-guide/about-us/#lifecycle-of-fits2db","title":"Lifecycle of fits2db","text":"<p><pre><code>flowchart LR\n    A(Create \\n Config File) --&gt; |Build \\n database| B(Tables are filled \\n with data and\n    Database is \\n ready to use)\n\n    B --&gt; C(New Data arrives \\n or\n    Data changes)\n\n    C --&gt; |Run update \\n command| B\n    C --&gt; |Optional \\n command| J(Remove \\n everthing)\n</code></pre> This high-level overview illustrates the simplicity and efficiency of managing FITS files with fits2db, from initial setup to ongoing updates, all while maintaining robust metadata management to keep your data organized and accessible.</p>"},{"location":"user-guide/about-us/#how-the-meta-tables-are-connected","title":"How the Meta tables are connected","text":"<p>The automatic created Metadata tables are to keep track of the files that are uploaded. This is done so that if you run update not everything gets rewritten but only the changed/new files get updated while the rest stays the same.</p> <p>Here are the Metadata table descriptions:</p> <p><pre><code>erDiagram\n    FITS2DB_META {\n        int id PK\n        varchar filename\n        varchar file_path\n        datetime file_last_mutated\n    }\n\n    FITS2DB_TABLE_META {\n        int id PK\n        varchar description\n        varchar tablename\n        int row_cnt\n        int col_cnt\n    }\n\n    YOUR_TABLE {\n        int id PK\n        varchar your_data\n        int metadata_id FK\n    }\n\n    YOUR_TABLE_META {\n        int id PK\n        text keyword\n        text value\n        int metadata_id FK\n    }\n\n    FITS2DB_META ||--|| FITS2DB_TABLE_META : \"foreign_id\"\n    FITS2DB_META ||--o| YOUR_TABLE : \"foreign_id\"\n    FITS2DB_TABLE_META ||--o| YOUR_TABLE_META : \"metadata_id\"</code></pre> This module defines the SQLAlchemy ORM models for the metadata tables used in the database. The models include:</p> <ul> <li>Fits2DbMeta: Represents metadata for FITS files.</li> <li>Fits2DbTableMeta: Represents metadata for tables related to FITS files.</li> </ul> <p>The relationships between these tables are visualized in the diagram above.</p>"},{"location":"user-guide/about-us/#fits2db.adapters.meta.Fits2DbMeta","title":"<code>Fits2DbMeta</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy ORM model representing the FITS2DB_META table.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Primary key, auto-incremented.</p> <code>filename</code> <code>str</code> <p>Name of the FITS file.</p> <code>filepath</code> <code>str</code> <p>Path to the FITS file.</p> <code>last_db_update</code> <code>datetime</code> <p>Timestamp of the last database update.</p> <code>last_file_mutation</code> <code>datetime</code> <p>Timestamp of the last file modification.</p> <code>tables</code> <code>relationship</code> <p>Relationship to the Fits2DbTableMeta objects     associated with this file.</p> Source code in <code>fits2db/adapters/meta.py</code> <pre><code>class Fits2DbMeta(Base):\n    \"\"\"\n    SQLAlchemy ORM model representing the FITS2DB_META table.\n\n    Attributes:\n        id (int): Primary key, auto-incremented.\n        filename (str): Name of the FITS file.\n        filepath (str): Path to the FITS file.\n        last_db_update (datetime): Timestamp of the last database update.\n        last_file_mutation (datetime): Timestamp of the last file modification.\n        tables (relationship): Relationship to the Fits2DbTableMeta objects\n                associated with this file.\n\n    \"\"\"\n\n    __tablename__ = \"fits2db_meta\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    filename = Column(Text)\n    filepath = Column(Text)\n    last_db_update = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n        onupdate=datetime.now(timezone.utc),\n    )\n    last_file_mutation = Column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n    )\n    # Relationship to associate files with their tables\n    tables = relationship(\n        \"Fits2DbTableMeta\",\n        back_populates=\"file_meta\",\n        cascade=\"all, delete-orphan\",\n    )\n</code></pre>"},{"location":"user-guide/installation/","title":"fits2db installation guide","text":"<p>It is recommended to use a virtual environment when running this tool. Utilizing <code>venv</code> or <code>conda</code> ensures a clean and isolated environment, which is a best practice.</p>"},{"location":"user-guide/installation/#install-from-pypi","title":"Install from <code>PYPI</code>","text":"<p>For Versions &lt;= 0.0.3 fits2db can be installed from PYPI. Newer Versions must be installed from source</p> <p>You can use pip to install this library </p> <p><pre><code>pip install fits2db\n</code></pre> or for specific version use  <pre><code>pip install fits2db==&lt;your_version&gt; \n</code></pre></p>"},{"location":"user-guide/installation/#install-from-source","title":"Install from source","text":"<p>Clone this repo to your local machine. Once cloned navigate to the <code>root</code> directory of this project and run in your python environment </p> <p>Build install with pip install</p> <pre><code>pip install .\n</code></pre> <p>With this the <code>fits2db</code> lib should be installed. You can test if its properly installed by running the version command to check if you got the right version cmd<pre><code>&gt;fits2db --version\n&gt;&gt;&gt; fits2db, version 0.1.0\n</code></pre></p> <p>Alternatively it can be installed from git with  <pre><code>pip install git+https://github.com/pmodwrc/fits2db.git@main\n</code></pre> For this method a local <code>git</code> client installation is required.</p> <p>Happy coding </p>"},{"location":"user-guide/overview/","title":"fits2db User Guide","text":""},{"location":"user-guide/overview/#overview","title":"Overview","text":"<p><code>fits2db</code> is a command-line interface (CLI) tool designed to load and update tables from FITS (Flexible Image Transport System) files into an SQL database. This tool is particularly useful for managing large datasets in astronomy and other fields that utilize FITS files for data storage.</p>"},{"location":"user-guide/overview/#key-features","title":"Key Features","text":"<ul> <li>Load FITS file data into SQL databases.</li> <li>Automatically create and initialize database tables based on FITS file contents.</li> <li>Append new data and update existing entries efficiently.</li> <li>Maintain metadata tables describing the columns and FITS file information.</li> </ul>"},{"location":"user-guide/overview/#installation","title":"Installation","text":"<p>To install fits2db, follow these steps:</p> <ol> <li> <p>Dependencies: Ensure you have Python installed on your system.     The Required libraries will be installed alongside fits2db.     These dependencies include libraries like astropy for handling FITS files and SQLAlchemy for database interactions. </p> </li> <li> <p>Install fits2db:</p> </li> </ol> pip installation<pre><code>pip install fits2db\n</code></pre>"},{"location":"user-guide/overview/#configuration","title":"Configuration","text":"<p>Before using fits2db, you need to create a configuration yaml file. This file contains information about your database and the FITS files you want to use.</p> Example config file <p>Example config file Example yaml<pre><code># Your db configs:\ndatabase:\ntype: mysql\nhost: localhost\nuser: user\npassword: userpassword\ndb_name: fitsdata\nport: 3306\n\n# Your fits files\nfits_files:\npaths:\n    - tests\\unit\\data\\2021-07-07_L1a.fits\n    - test\\unit\\data # If just a path is given it will search recursive for fits files\n\ndelete_rows_from_missing_tables: False # [optional] by default false\n\ntables: # The tables from the fitsfiles you want to upload\n    - name: HOUSEKEEPING\n      date_column: timetsamp\n    - name: JTSIM_BROADCAST\n      date_column: timestamp\n</code></pre></p>"},{"location":"user-guide/overview/#configuration-parameters","title":"Configuration Parameters","text":"<ul> <li> <p>database: Specifies the type and name of your database.</p> <ul> <li>type: Type of database (currently just mysql).</li> <li>host: Db host</li> <li>user: User of db that has rights to read write create and drop tables</li> <li>password: pw of user</li> <li>port: default is 3306 if not specified</li> </ul> </li> <li> <p>fits_files: A list of directories or paths containing your FITS files.</p> <ul> <li>path: Path to the directory with FITS files.</li> <li>delete_rows_from_missing_tables: Sets wheter entries in tables not specified in the config on updated files are removed during the update of a file.</li> <li>tables: Names of the tables to create and populate in the database.<ul> <li>date_column: column in the table that contains a datetime value. This column will be uploaded to the database as type 'datetime'</li> </ul> </li> </ul> </li> </ul>"},{"location":"user-guide/overview/#usage","title":"Usage","text":""},{"location":"user-guide/overview/#initial-setup","title":"Initial Setup","text":"<p>To initialize the database and load data from the FITS files, use the rebuild command.  This command creates the necessary tables in the SQL database and populates them with data from the FITS files.</p>"},{"location":"user-guide/overview/#additional-resources","title":"Additional Resources","text":"<ul> <li>FITS File Format Documentation</li> <li>SQLAlchemy Documentation</li> </ul>"},{"location":"user-guide/overview/#license-and-contributions","title":"License and Contributions","text":"<p>fits2db is an open-source project licensed under the MIT License. Contributions are welcome! To contribute, follow these steps:</p> <ul> <li>Fork the repository.</li> <li>Create a new branch for your feature or bugfix.</li> <li>Submit a pull request with a detailed description of your changes.</li> </ul> <p>For more details, see the contribution guide.</p>"},{"location":"user-guide/usage/","title":"Usage","text":""},{"location":"user-guide/usage/#getting-help","title":"Getting help","text":"<p>If you need some information about a command you can always pass the <code>--help</code> flag to get some information.</p>"},{"location":"user-guide/usage/#generate-a-config-file","title":"Generate a config file","text":"<p>First you can generate a template config file: <pre><code>$ fits2db init\n</code></pre></p> <p>Note</p> <p>you can also pass a path to the command to choose where the config file should be generated and what the name of the file should be:</p> <p><pre><code>$ fits2db init &lt;folder_path&gt;\n</code></pre> this generates a file <code>config.yml</code> under your given path <pre><code>$ fits2db init &lt;example/path/config_test.yaml&gt;\n</code></pre> this gernerates the config file with the name <code>config_test.yaml</code>.</p>"},{"location":"user-guide/usage/#make-your-changes","title":"Make your changes","text":"<p>In the <code>config.yml</code> file you now can change the variables needed. </p> <p>Fill in the database credentials: <pre><code>database:\n  type: mysql\n  host: localhost\n  user: user\n  password: password\n  db_name: test_db\n  port: 3306\n</code></pre> and add some paths for your fits files</p> <pre><code>fits_files:\n  paths:\n    - path/to_your_file/2021-07-07_L1a.fits\n    - path_to_your_folder\n\n# Delete rows from above listed files from tables which are not listed below. By default False\ndelete_rows_from_missing_tables: True\n\ntables:\n    - name: HOUSEKEEPING\n      date_column: timestamp # This column will be interpreted as a datetime variable\n    - name: IRRADIANCE # If no table name given it will use the orignal name\n      date_column: irradiance_timeutc\n</code></pre> <p>Note</p> <p>if a folder is given all fits files under this folder will be taken recursively for upload.</p> <p>Note</p> <p>if the date column is not 'timestamp', a copy of the date column called 'timestamp' is created. This is due to backward compatibility reasons.</p>"},{"location":"user-guide/usage/#check-if-the-right-files-are-taken","title":"Check if the right files are taken","text":"<p>You can check if you get the right fits files with  <pre><code>$ fits2db files &lt;path_to_config_file&gt;\n</code></pre> this command shows all files it will consider uploading in your terminal and at the end shows the number of files.</p> <p>Note</p> <p>If you don't add an path the cli looks for the config file in the same folder as you are currently in.</p>"},{"location":"user-guide/usage/#inspect-available-tables","title":"Inspect available Tables","text":"<p>If you want to see what tables are available in your fits files you can run  <pre><code>$ fits2db tables &lt;path_to_config_file&gt; \n</code></pre> this will get you a summary of all tables</p> <p>Tip</p> <p>If you want to see if the tables are available in all files just run the sam command with the matrix flag <pre><code>$ fits2db tables &lt;path_to_config_file&gt; -m\n</code></pre> this will show the reult in the terminal. If you want to have and excel or csv use <pre><code>$ fits2db tables &lt;path_to_config_file&gt; -m --excel --filename path/your_filename.xlsx\n</code></pre></p>"},{"location":"user-guide/usage/#build-db","title":"Build db","text":"<p>Now upload the data into our data base we use the build command <pre><code>$ fits2db build &lt;path_to_config_file&gt; \n</code></pre> this will upload all the fits tables into your data base and create the meta tables to keep track on changes of the files When running thei build command the user is prompted to confirm the action to remove all existing data from the configured database. If the user denies, the build command is aborted.</p> <p>Warning</p> <p>If you rerun the build command it acts as an reset. It will drop the tables and reupload all data to have a fresh start. This is only recommend to use when you lost track of some changes in the data you have done manually and you are not sure you corrupted the data.</p>"},{"location":"user-guide/usage/#update-db","title":"Update db","text":"<p>Once builded and you get new files or changes you can update the database.  This command will check if there a new files in your defnied folders and  upload them to the db. If the timestamp of your file changed to a newer date. Like when you changed a file it will also update this file to the  newer version. This way the fits files and the db stay in sync. To update just run </p> <pre><code>$ fits2db update &lt;path_to_config_file&gt; \n</code></pre> <p>Note</p> <p>If you want to add new tables from a already updated file, you can use the <code>-f</code> flag to force update all files specified in the config. This can for example be used to add additional tables from a already uploaded file to the database.</p>"},{"location":"user-guide/usage/#remove-files-from-tables","title":"Remove files from tables","text":"<p>With the <code>remove_rows_from_missing_tables</code> option, one can remove entries form columns. For example if a upladed file has entries in Table <code>a</code> and <code>b</code> and the update command is excecuted with only the  table <code>a</code> configured and <code>remove_rows_from_missing_tables</code> set to True, then the rows from all configured files from  table <code>b</code> will be removed.</p>"}]}